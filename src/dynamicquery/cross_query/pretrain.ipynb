{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "653f0986-baec-456f-a937-41fb25bcc2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import utils\n",
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7093d89-7d06-4a5b-92f2-c3d50385e3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clef2021PretrainingDataset(TensorDataset):\n",
    "    def __init__(self, \n",
    "                 encode_fn, \n",
    "                 claims,\n",
    "                 claim_embeddings,\n",
    "                 n_negatives=5):\n",
    "        self.n_negatives = n_negatives\n",
    "        self.claim_embeddings = claim_embeddings\n",
    "        self.claims = claims\n",
    "        self.claims[\"encoded_vclaim\"] = self.claims.vclaim.apply(encode_fn)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.claims)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        negative_idx = np.random.randint(len(self.claim_embeddings), size=(self.n_negatives,))\n",
    "        negative_embs = self.claim_embeddings[negative_idx]\n",
    "        positive_emb = self.claim_embeddings[index]\n",
    "        embs = np.concatenate([np.expand_dims(positive_emb, 0), negative_embs], 0)\n",
    "\n",
    "        positive_tokens = self.claims[\"encoded_vclaim\"][index]\n",
    "        positive_inpt = (np.array(positive_tokens[\"input_ids\"]), np.array(positive_tokens[\"attention_mask\"]))\n",
    "\n",
    "        return (positive_inpt, embs)\n",
    "    \n",
    "    \n",
    "def get_clef2021_pretraining_dataloader(encode_fn, \n",
    "                            claims, \n",
    "                            claim_embeddings,\n",
    "                            n_negatives=5,\n",
    "                            params={'batch_size':32, 'shuffle':True}):\n",
    "    dataset = Clef2021PretrainingDataset(encode_fn, \n",
    "                              claims, \n",
    "                              claim_embeddings,\n",
    "                              n_negatives=n_negatives)\n",
    "    # return dataset\n",
    "    return DataLoader(dataset, **params)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e609675-bbc1-43a4-a44a-3c2aef866d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/mshlis/Projects/RIET/DynamicQuery\"\n",
    "exp_path = os.path.join(base_path, \"experiments/finetune_st5_large_claims_negs\")\n",
    "train_neg_path = os.path.join(exp_path, \"negative_embs_train.npy\")\n",
    "dev_neg_path = os.path.join(exp_path, \"negative_embs_dev.npy\")\n",
    "emb_path = os.path.join(exp_path, \"claim_embs.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92395a0b-6e95-4737-a468-48160b480a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing ExtendedRobertaForExternalClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing ExtendedRobertaForExternalClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ExtendedRobertaForExternalClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ExtendedRobertaForExternalClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.adapter_layer.1.weight', 'roberta.encoder.adapter_layer.10.weight', 'roberta.encoder.adapter_layer.4.bias', 'roberta.encoder.adapter_layer.8.weight', 'roberta.encoder.adapter_layer.5.bias', 'roberta.encoder.adapter_layer.7.weight', 'roberta.encoder.adapter_layer.8.bias', 'roberta.encoder.adapter_layer.6.bias', 'roberta.encoder.adapter_layer.0.bias', 'roberta.encoder.adapter_layer.9.bias', 'roberta.encoder.adapter_layer.6.weight', 'roberta.encoder.adapter_layer.3.bias', 'roberta.encoder.adapter_layer.2.bias', 'roberta.encoder.adapter_layer.11.bias', 'roberta.encoder.adapter_layer.1.bias', 'roberta.encoder.adapter_layer.9.weight', 'roberta.encoder.adapter_layer.10.bias', 'roberta.encoder.adapter_layer.5.weight', 'roberta.encoder.adapter_layer.0.weight', 'roberta.encoder.adapter_layer.7.bias', 'roberta.encoder.adapter_layer.2.weight', 'roberta.encoder.adapter_layer.4.weight', 'roberta.encoder.adapter_layer.11.weight', 'classifier.bias', 'roberta.encoder.adapter_layer.3.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import extended_roberta as roberta\n",
    "from transformers import AutoTokenizer\n",
    "from functools import partial\n",
    "import importlib\n",
    "importlib.reload(roberta)\n",
    "\n",
    "MAX_LENGTH = 192\n",
    "\n",
    "model_str = \"roberta-base\"\n",
    "model = roberta.ExtendedRobertaForExternalClassification.from_pretrained(model_str)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_str)\n",
    "tokenize = partial(tokenizer, **dict(\n",
    "    truncation=True, \n",
    "    max_length=MAX_LENGTH, \n",
    "    padding=\"max_length\", \n",
    "    return_attention_mask=True\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb702fa9-ef41-4f68-b09d-ef839e2afa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_ids = np.load(train_neg_path)\n",
    "dev_neg_ids = np.load(dev_neg_path)\n",
    "neg_embs = np.load(emb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4b7c0a6-1d01-41a2-8e71-97dabce4336a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((999, 13824), (13825, 768))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_ids.shape, neg_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffcaf466-8b8a-4fb8-83af-d11a44f6d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(base_path)\n",
    "\n",
    "# Claim Data\n",
    "tweets, test_tweets = utils.get_tweets()\n",
    "test_tweets = test_tweets[1:]\n",
    "train_conns, dev_conns, test_conns = utils.get_qrels()\n",
    "claims = utils.get_claims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "817212db-4b4a-42b9-a577-45f85696b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dl = get_clef2021_pretraining_dataloader(\n",
    "    tokenize, \n",
    "    claims, \n",
    "    neg_embs,\n",
    "    n_negatives=5,\n",
    "    # neg_ids[:,:5],\n",
    "    params={'batch_size':BATCH_SIZE, 'shuffle':True})\n",
    "\n",
    "dev_dl = get_clef2021_pretraining_dataloader(\n",
    "    tokenize, \n",
    "    claims, \n",
    "    neg_embs,\n",
    "    n_negatives=5,\n",
    "    # dev_neg_ids[:,:5],\n",
    "    params={'batch_size':BATCH_SIZE, 'shuffle':False}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea92077f-50cb-4046-aab4-924578de875c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'train' from '/home/mshlis/Projects/RIET/DynamicQuery/src/dynamicquery/cross_query/train.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import train\n",
    "importlib.reload(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72a4e5d7-b0ce-4eb8-aa5b-4ea03bc0c7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50c610a5-aabb-4a40-9521-75e08b088cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71741819-4a21-4ca8-ac95-d4bf5073579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28fdac73-ae9c-45cc-a392-1102fad07a3c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN [1,     5] loss: 1.799\n",
      "TRAIN [1,    10] loss: 1.803\n",
      "TRAIN [1,    15] loss: 1.799\n",
      "TRAIN [1,    20] loss: 1.813\n",
      "TRAIN [1,    25] loss: 1.798\n",
      "TRAIN [1,    30] loss: 1.791\n",
      "TRAIN [1,    35] loss: 1.796\n",
      "TRAIN [1,    40] loss: 1.795\n",
      "TRAIN [1,    45] loss: 1.788\n",
      "TRAIN [1,    50] loss: 1.783\n",
      "TRAIN [1,    55] loss: 1.794\n",
      "TRAIN [1,    60] loss: 1.797\n",
      "TRAIN [1,    65] loss: 1.806\n",
      "TRAIN [1,    70] loss: 1.787\n",
      "TRAIN [1,    75] loss: 1.792\n",
      "TRAIN [1,    80] loss: 1.801\n",
      "TRAIN [1,    85] loss: 1.788\n",
      "TRAIN [1,    90] loss: 1.809\n",
      "TRAIN [1,    95] loss: 1.793\n",
      "TRAIN [1,   100] loss: 1.784\n",
      "TRAIN [1,   105] loss: 1.785\n",
      "TRAIN [1,   110] loss: 1.796\n",
      "TRAIN [1,   115] loss: 1.795\n",
      "TRAIN [1,   120] loss: 1.791\n",
      "TRAIN [1,   125] loss: 1.792\n",
      "TRAIN [1,   130] loss: 1.785\n",
      "TRAIN [1,   135] loss: 1.788\n",
      "TRAIN [1,   140] loss: 1.786\n",
      "TRAIN [1,   145] loss: 1.798\n",
      "TRAIN [1,   150] loss: 1.808\n",
      "TRAIN [1,   155] loss: 1.793\n",
      "TRAIN [1,   160] loss: 1.797\n",
      "TRAIN [1,   165] loss: 1.806\n",
      "TRAIN [1,   170] loss: 1.791\n",
      "TRAIN [1,   175] loss: 1.771\n",
      "TRAIN [1,   180] loss: 1.790\n",
      "TRAIN [1,   185] loss: 1.793\n",
      "TRAIN [1,   190] loss: 1.796\n",
      "TRAIN [1,   195] loss: 1.775\n",
      "TRAIN [1,   200] loss: 1.778\n",
      "TRAIN [1,   205] loss: 1.760\n",
      "TRAIN [1,   210] loss: 1.803\n",
      "TRAIN [1,   215] loss: 1.777\n",
      "TRAIN [1,   220] loss: 1.775\n",
      "TRAIN [1,   225] loss: 1.790\n",
      "TRAIN [1,   230] loss: 1.793\n",
      "TRAIN [1,   235] loss: 1.789\n",
      "TRAIN [1,   240] loss: 1.781\n",
      "TRAIN [1,   245] loss: 1.765\n",
      "TRAIN [1,   250] loss: 1.793\n",
      "TRAIN [1,   255] loss: 1.801\n",
      "TRAIN [1,   260] loss: 1.789\n",
      "TRAIN [1,   265] loss: 1.779\n",
      "TRAIN [1,   270] loss: 1.768\n",
      "TRAIN [1,   275] loss: 1.775\n",
      "TRAIN [1,   280] loss: 1.781\n",
      "TRAIN [1,   285] loss: 1.765\n",
      "TRAIN [1,   290] loss: 1.809\n",
      "TRAIN [1,   295] loss: 1.787\n",
      "TRAIN [1,   300] loss: 1.769\n",
      "TRAIN [1,   305] loss: 1.757\n",
      "TRAIN [1,   310] loss: 1.759\n",
      "TRAIN [1,   315] loss: 1.772\n",
      "TRAIN [1,   320] loss: 1.759\n",
      "TRAIN [1,   325] loss: 1.779\n",
      "TRAIN [1,   330] loss: 1.769\n",
      "TRAIN [1,   335] loss: 1.748\n",
      "TRAIN [1,   340] loss: 1.745\n",
      "TRAIN [1,   345] loss: 1.776\n",
      "TRAIN [1,   350] loss: 1.751\n",
      "TRAIN [1,   355] loss: 1.748\n",
      "TRAIN [1,   360] loss: 1.758\n",
      "TRAIN [1,   365] loss: 1.752\n",
      "TRAIN [1,   370] loss: 1.759\n",
      "TRAIN [1,   375] loss: 1.767\n",
      "TRAIN [1,   380] loss: 1.759\n",
      "TRAIN [1,   385] loss: 1.750\n",
      "TRAIN [1,   390] loss: 1.749\n",
      "TRAIN [1,   395] loss: 1.737\n",
      "TRAIN [1,   400] loss: 1.711\n",
      "TRAIN [1,   405] loss: 1.713\n",
      "TRAIN [1,   410] loss: 1.731\n",
      "TRAIN [1,   415] loss: 1.727\n",
      "TRAIN [1,   420] loss: 1.728\n",
      "TRAIN [1,   425] loss: 1.722\n",
      "TRAIN [1,   430] loss: 1.707\n",
      "DEV [1,   433] loss: 1.675\n",
      "TRAIN [2,     5] loss: 1.711\n",
      "TRAIN [2,    10] loss: 1.736\n",
      "TRAIN [2,    15] loss: 1.681\n",
      "TRAIN [2,    20] loss: 1.706\n",
      "TRAIN [2,    25] loss: 1.693\n",
      "TRAIN [2,    30] loss: 1.690\n",
      "TRAIN [2,    35] loss: 1.670\n",
      "TRAIN [2,    40] loss: 1.654\n",
      "TRAIN [2,    45] loss: 1.705\n",
      "TRAIN [2,    50] loss: 1.647\n",
      "TRAIN [2,    55] loss: 1.649\n",
      "TRAIN [2,    60] loss: 1.638\n",
      "TRAIN [2,    65] loss: 1.644\n",
      "TRAIN [2,    70] loss: 1.611\n",
      "TRAIN [2,    75] loss: 1.635\n",
      "TRAIN [2,    80] loss: 1.619\n",
      "TRAIN [2,    85] loss: 1.573\n",
      "TRAIN [2,    90] loss: 1.599\n",
      "TRAIN [2,    95] loss: 1.605\n",
      "TRAIN [2,   100] loss: 1.594\n",
      "TRAIN [2,   105] loss: 1.609\n",
      "TRAIN [2,   110] loss: 1.576\n",
      "TRAIN [2,   115] loss: 1.570\n",
      "TRAIN [2,   120] loss: 1.553\n",
      "TRAIN [2,   125] loss: 1.535\n",
      "TRAIN [2,   130] loss: 1.514\n",
      "TRAIN [2,   135] loss: 1.516\n",
      "TRAIN [2,   140] loss: 1.515\n",
      "TRAIN [2,   145] loss: 1.531\n",
      "TRAIN [2,   150] loss: 1.485\n",
      "TRAIN [2,   155] loss: 1.588\n",
      "TRAIN [2,   160] loss: 1.519\n",
      "TRAIN [2,   165] loss: 1.484\n",
      "TRAIN [2,   170] loss: 1.481\n",
      "TRAIN [2,   175] loss: 1.484\n",
      "TRAIN [2,   180] loss: 1.509\n",
      "TRAIN [2,   185] loss: 1.426\n",
      "TRAIN [2,   190] loss: 1.560\n",
      "TRAIN [2,   195] loss: 1.512\n",
      "TRAIN [2,   200] loss: 1.384\n",
      "TRAIN [2,   205] loss: 1.468\n",
      "TRAIN [2,   210] loss: 1.369\n",
      "TRAIN [2,   215] loss: 1.508\n",
      "TRAIN [2,   220] loss: 1.439\n",
      "TRAIN [2,   225] loss: 1.441\n",
      "TRAIN [2,   230] loss: 1.459\n",
      "TRAIN [2,   235] loss: 1.408\n",
      "TRAIN [2,   240] loss: 1.405\n",
      "TRAIN [2,   245] loss: 1.399\n",
      "TRAIN [2,   250] loss: 1.352\n",
      "TRAIN [2,   255] loss: 1.431\n",
      "TRAIN [2,   260] loss: 1.367\n",
      "TRAIN [2,   265] loss: 1.390\n",
      "TRAIN [2,   270] loss: 1.419\n",
      "TRAIN [2,   275] loss: 1.357\n",
      "TRAIN [2,   280] loss: 1.411\n",
      "TRAIN [2,   285] loss: 1.421\n",
      "TRAIN [2,   290] loss: 1.374\n",
      "TRAIN [2,   295] loss: 1.316\n",
      "TRAIN [2,   300] loss: 1.331\n",
      "TRAIN [2,   305] loss: 1.364\n",
      "TRAIN [2,   310] loss: 1.260\n",
      "TRAIN [2,   315] loss: 1.383\n",
      "TRAIN [2,   320] loss: 1.329\n",
      "TRAIN [2,   325] loss: 1.333\n",
      "TRAIN [2,   330] loss: 1.352\n",
      "TRAIN [2,   335] loss: 1.338\n",
      "TRAIN [2,   340] loss: 1.354\n",
      "TRAIN [2,   345] loss: 1.328\n",
      "TRAIN [2,   350] loss: 1.360\n",
      "TRAIN [2,   355] loss: 1.258\n",
      "TRAIN [2,   360] loss: 1.292\n",
      "TRAIN [2,   365] loss: 1.330\n",
      "TRAIN [2,   370] loss: 1.302\n",
      "TRAIN [2,   375] loss: 1.329\n",
      "TRAIN [2,   380] loss: 1.275\n",
      "TRAIN [2,   385] loss: 1.360\n",
      "TRAIN [2,   390] loss: 1.312\n",
      "TRAIN [2,   395] loss: 1.334\n",
      "TRAIN [2,   400] loss: 1.350\n",
      "TRAIN [2,   405] loss: 1.287\n",
      "TRAIN [2,   410] loss: 1.249\n",
      "TRAIN [2,   415] loss: 1.286\n",
      "TRAIN [2,   420] loss: 1.267\n",
      "TRAIN [2,   425] loss: 1.265\n",
      "TRAIN [2,   430] loss: 1.215\n",
      "DEV [2,   433] loss: 1.149\n",
      "TRAIN [3,     5] loss: 1.293\n",
      "TRAIN [3,    10] loss: 1.330\n",
      "TRAIN [3,    15] loss: 1.267\n",
      "TRAIN [3,    20] loss: 1.287\n",
      "TRAIN [3,    25] loss: 1.225\n",
      "TRAIN [3,    30] loss: 1.194\n",
      "TRAIN [3,    35] loss: 1.232\n",
      "TRAIN [3,    40] loss: 1.230\n",
      "TRAIN [3,    45] loss: 1.181\n",
      "TRAIN [3,    50] loss: 1.186\n",
      "TRAIN [3,    55] loss: 1.257\n",
      "TRAIN [3,    60] loss: 1.160\n",
      "TRAIN [3,    65] loss: 1.200\n",
      "TRAIN [3,    70] loss: 1.261\n",
      "TRAIN [3,    75] loss: 1.185\n",
      "TRAIN [3,    80] loss: 1.276\n",
      "TRAIN [3,    85] loss: 1.195\n",
      "TRAIN [3,    90] loss: 1.236\n",
      "TRAIN [3,    95] loss: 1.189\n",
      "TRAIN [3,   100] loss: 1.218\n",
      "TRAIN [3,   105] loss: 1.128\n",
      "TRAIN [3,   110] loss: 1.216\n",
      "TRAIN [3,   115] loss: 1.141\n",
      "TRAIN [3,   120] loss: 1.093\n",
      "TRAIN [3,   125] loss: 1.225\n",
      "TRAIN [3,   130] loss: 1.257\n",
      "TRAIN [3,   135] loss: 1.230\n",
      "TRAIN [3,   140] loss: 1.121\n",
      "TRAIN [3,   145] loss: 1.153\n",
      "TRAIN [3,   150] loss: 1.119\n",
      "TRAIN [3,   155] loss: 1.163\n",
      "TRAIN [3,   160] loss: 1.145\n",
      "TRAIN [3,   165] loss: 1.157\n",
      "TRAIN [3,   170] loss: 1.270\n",
      "TRAIN [3,   175] loss: 1.165\n",
      "TRAIN [3,   180] loss: 1.114\n",
      "TRAIN [3,   185] loss: 1.222\n",
      "TRAIN [3,   190] loss: 1.164\n",
      "TRAIN [3,   195] loss: 1.176\n",
      "TRAIN [3,   200] loss: 1.223\n",
      "TRAIN [3,   205] loss: 1.142\n",
      "TRAIN [3,   210] loss: 1.109\n",
      "TRAIN [3,   215] loss: 1.110\n",
      "TRAIN [3,   220] loss: 1.065\n",
      "TRAIN [3,   225] loss: 1.147\n",
      "TRAIN [3,   230] loss: 1.075\n",
      "TRAIN [3,   235] loss: 1.153\n",
      "TRAIN [3,   240] loss: 1.111\n",
      "TRAIN [3,   245] loss: 1.141\n",
      "TRAIN [3,   250] loss: 1.129\n",
      "TRAIN [3,   255] loss: 1.053\n",
      "TRAIN [3,   260] loss: 1.121\n",
      "TRAIN [3,   265] loss: 1.153\n",
      "TRAIN [3,   270] loss: 1.232\n",
      "TRAIN [3,   275] loss: 1.035\n",
      "TRAIN [3,   280] loss: 1.086\n",
      "TRAIN [3,   285] loss: 1.111\n",
      "TRAIN [3,   290] loss: 1.163\n",
      "TRAIN [3,   295] loss: 1.115\n",
      "TRAIN [3,   300] loss: 1.062\n",
      "TRAIN [3,   305] loss: 1.093\n",
      "TRAIN [3,   310] loss: 1.089\n",
      "TRAIN [3,   315] loss: 1.148\n",
      "TRAIN [3,   320] loss: 1.052\n",
      "TRAIN [3,   325] loss: 1.110\n",
      "TRAIN [3,   330] loss: 1.189\n",
      "TRAIN [3,   335] loss: 1.077\n",
      "TRAIN [3,   340] loss: 1.088\n",
      "TRAIN [3,   345] loss: 1.066\n",
      "TRAIN [3,   350] loss: 1.122\n",
      "TRAIN [3,   355] loss: 1.039\n",
      "TRAIN [3,   360] loss: 1.051\n",
      "TRAIN [3,   365] loss: 1.091\n",
      "TRAIN [3,   370] loss: 1.049\n",
      "TRAIN [3,   375] loss: 1.178\n",
      "TRAIN [3,   380] loss: 1.055\n",
      "TRAIN [3,   385] loss: 1.146\n",
      "TRAIN [3,   390] loss: 1.091\n",
      "TRAIN [3,   395] loss: 1.016\n",
      "TRAIN [3,   400] loss: 1.030\n",
      "TRAIN [3,   405] loss: 1.103\n",
      "TRAIN [3,   410] loss: 1.191\n",
      "TRAIN [3,   415] loss: 1.040\n",
      "TRAIN [3,   420] loss: 1.012\n",
      "TRAIN [3,   425] loss: 1.192\n",
      "TRAIN [3,   430] loss: 1.050\n",
      "DEV [3,   433] loss: 0.900\n",
      "TRAIN [4,     5] loss: 1.051\n",
      "TRAIN [4,    10] loss: 1.073\n",
      "TRAIN [4,    15] loss: 1.025\n",
      "TRAIN [4,    20] loss: 0.993\n",
      "TRAIN [4,    25] loss: 1.078\n",
      "TRAIN [4,    30] loss: 1.029\n",
      "TRAIN [4,    35] loss: 1.012\n",
      "TRAIN [4,    40] loss: 1.047\n",
      "TRAIN [4,    45] loss: 0.971\n",
      "TRAIN [4,    50] loss: 1.007\n",
      "TRAIN [4,    55] loss: 1.069\n",
      "TRAIN [4,    60] loss: 0.908\n",
      "TRAIN [4,    65] loss: 0.993\n",
      "TRAIN [4,    70] loss: 1.028\n",
      "TRAIN [4,    75] loss: 1.030\n",
      "TRAIN [4,    80] loss: 1.032\n",
      "TRAIN [4,    85] loss: 1.022\n",
      "TRAIN [4,    90] loss: 1.095\n",
      "TRAIN [4,    95] loss: 0.975\n",
      "TRAIN [4,   100] loss: 0.928\n",
      "TRAIN [4,   105] loss: 0.997\n",
      "TRAIN [4,   110] loss: 0.994\n",
      "TRAIN [4,   115] loss: 0.988\n",
      "TRAIN [4,   120] loss: 1.000\n",
      "TRAIN [4,   125] loss: 1.081\n",
      "TRAIN [4,   130] loss: 1.043\n",
      "TRAIN [4,   135] loss: 0.972\n",
      "TRAIN [4,   140] loss: 1.022\n",
      "TRAIN [4,   145] loss: 1.050\n",
      "TRAIN [4,   150] loss: 0.915\n",
      "TRAIN [4,   155] loss: 1.016\n",
      "TRAIN [4,   160] loss: 1.017\n",
      "TRAIN [4,   165] loss: 1.004\n",
      "TRAIN [4,   170] loss: 1.015\n",
      "TRAIN [4,   175] loss: 0.975\n",
      "TRAIN [4,   180] loss: 1.070\n",
      "TRAIN [4,   185] loss: 1.026\n",
      "TRAIN [4,   190] loss: 1.005\n",
      "TRAIN [4,   195] loss: 0.977\n",
      "TRAIN [4,   200] loss: 0.989\n",
      "TRAIN [4,   205] loss: 1.091\n",
      "TRAIN [4,   210] loss: 0.952\n",
      "TRAIN [4,   215] loss: 0.958\n",
      "TRAIN [4,   220] loss: 0.941\n",
      "TRAIN [4,   225] loss: 0.976\n",
      "TRAIN [4,   230] loss: 0.970\n",
      "TRAIN [4,   235] loss: 0.967\n",
      "TRAIN [4,   240] loss: 0.990\n",
      "TRAIN [4,   245] loss: 0.978\n",
      "TRAIN [4,   250] loss: 0.956\n",
      "TRAIN [4,   255] loss: 0.985\n",
      "TRAIN [4,   260] loss: 0.975\n",
      "TRAIN [4,   265] loss: 0.939\n",
      "TRAIN [4,   270] loss: 1.005\n",
      "TRAIN [4,   275] loss: 0.969\n",
      "TRAIN [4,   280] loss: 0.893\n",
      "TRAIN [4,   285] loss: 0.965\n",
      "TRAIN [4,   290] loss: 0.947\n",
      "TRAIN [4,   295] loss: 1.005\n",
      "TRAIN [4,   300] loss: 0.901\n",
      "TRAIN [4,   305] loss: 0.902\n",
      "TRAIN [4,   310] loss: 0.937\n",
      "TRAIN [4,   315] loss: 0.940\n",
      "TRAIN [4,   320] loss: 0.947\n",
      "TRAIN [4,   325] loss: 0.963\n",
      "TRAIN [4,   330] loss: 0.939\n",
      "TRAIN [4,   335] loss: 0.966\n",
      "TRAIN [4,   340] loss: 0.983\n",
      "TRAIN [4,   345] loss: 0.909\n",
      "TRAIN [4,   350] loss: 0.991\n",
      "TRAIN [4,   355] loss: 0.919\n",
      "TRAIN [4,   360] loss: 0.920\n",
      "TRAIN [4,   365] loss: 0.945\n",
      "TRAIN [4,   370] loss: 0.907\n",
      "TRAIN [4,   375] loss: 0.893\n",
      "TRAIN [4,   380] loss: 0.837\n",
      "TRAIN [4,   385] loss: 0.865\n",
      "TRAIN [4,   390] loss: 0.927\n",
      "TRAIN [4,   395] loss: 0.898\n",
      "TRAIN [4,   400] loss: 0.840\n",
      "TRAIN [4,   405] loss: 0.944\n",
      "TRAIN [4,   410] loss: 0.961\n",
      "TRAIN [4,   415] loss: 0.925\n",
      "TRAIN [4,   420] loss: 0.785\n",
      "TRAIN [4,   425] loss: 0.864\n",
      "TRAIN [4,   430] loss: 0.898\n",
      "DEV [4,   433] loss: 0.675\n",
      "TRAIN [5,     5] loss: 0.802\n",
      "TRAIN [5,    10] loss: 0.779\n",
      "TRAIN [5,    15] loss: 0.958\n",
      "TRAIN [5,    20] loss: 0.915\n",
      "TRAIN [5,    25] loss: 0.819\n",
      "TRAIN [5,    30] loss: 0.808\n",
      "TRAIN [5,    35] loss: 0.854\n",
      "TRAIN [5,    40] loss: 0.897\n",
      "TRAIN [5,    45] loss: 0.784\n",
      "TRAIN [5,    50] loss: 0.773\n",
      "TRAIN [5,    55] loss: 0.804\n",
      "TRAIN [5,    60] loss: 0.756\n",
      "TRAIN [5,    65] loss: 0.794\n",
      "TRAIN [5,    70] loss: 0.817\n",
      "TRAIN [5,    75] loss: 0.784\n",
      "TRAIN [5,    80] loss: 0.838\n",
      "TRAIN [5,    85] loss: 0.724\n",
      "TRAIN [5,    90] loss: 0.738\n",
      "TRAIN [5,    95] loss: 0.804\n",
      "TRAIN [5,   100] loss: 0.747\n",
      "TRAIN [5,   105] loss: 0.924\n",
      "TRAIN [5,   110] loss: 0.791\n",
      "TRAIN [5,   115] loss: 0.780\n",
      "TRAIN [5,   120] loss: 0.861\n",
      "TRAIN [5,   125] loss: 0.827\n",
      "TRAIN [5,   130] loss: 0.799\n",
      "TRAIN [5,   135] loss: 0.796\n",
      "TRAIN [5,   140] loss: 0.876\n",
      "TRAIN [5,   145] loss: 0.834\n",
      "TRAIN [5,   150] loss: 0.781\n",
      "TRAIN [5,   155] loss: 0.691\n",
      "TRAIN [5,   160] loss: 0.811\n",
      "TRAIN [5,   165] loss: 0.734\n",
      "TRAIN [5,   170] loss: 0.786\n",
      "TRAIN [5,   175] loss: 0.782\n",
      "TRAIN [5,   180] loss: 0.810\n",
      "TRAIN [5,   185] loss: 0.731\n",
      "TRAIN [5,   190] loss: 0.738\n",
      "TRAIN [5,   195] loss: 0.777\n",
      "TRAIN [5,   200] loss: 0.789\n",
      "TRAIN [5,   205] loss: 0.727\n",
      "TRAIN [5,   210] loss: 0.704\n",
      "TRAIN [5,   215] loss: 0.777\n",
      "TRAIN [5,   220] loss: 0.826\n",
      "TRAIN [5,   225] loss: 0.789\n",
      "TRAIN [5,   230] loss: 0.759\n",
      "TRAIN [5,   235] loss: 0.745\n",
      "TRAIN [5,   240] loss: 0.699\n",
      "TRAIN [5,   245] loss: 0.762\n",
      "TRAIN [5,   250] loss: 0.729\n",
      "TRAIN [5,   255] loss: 0.676\n",
      "TRAIN [5,   260] loss: 0.670\n",
      "TRAIN [5,   265] loss: 0.723\n",
      "TRAIN [5,   270] loss: 0.809\n",
      "TRAIN [5,   275] loss: 0.652\n",
      "TRAIN [5,   280] loss: 0.653\n",
      "TRAIN [5,   285] loss: 0.660\n",
      "TRAIN [5,   290] loss: 0.781\n",
      "TRAIN [5,   295] loss: 0.791\n",
      "TRAIN [5,   300] loss: 0.705\n",
      "TRAIN [5,   305] loss: 0.695\n",
      "TRAIN [5,   310] loss: 0.735\n",
      "TRAIN [5,   315] loss: 0.683\n",
      "TRAIN [5,   320] loss: 0.653\n",
      "TRAIN [5,   325] loss: 0.670\n",
      "TRAIN [5,   330] loss: 0.702\n",
      "TRAIN [5,   335] loss: 0.605\n",
      "TRAIN [5,   340] loss: 0.624\n",
      "TRAIN [5,   345] loss: 0.633\n",
      "TRAIN [5,   350] loss: 0.698\n",
      "TRAIN [5,   355] loss: 0.644\n",
      "TRAIN [5,   360] loss: 0.694\n",
      "TRAIN [5,   365] loss: 0.663\n",
      "TRAIN [5,   370] loss: 0.678\n",
      "TRAIN [5,   375] loss: 0.680\n",
      "TRAIN [5,   380] loss: 0.635\n",
      "TRAIN [5,   385] loss: 0.804\n",
      "TRAIN [5,   390] loss: 0.608\n",
      "TRAIN [5,   395] loss: 0.621\n",
      "TRAIN [5,   400] loss: 0.728\n",
      "TRAIN [5,   405] loss: 0.591\n",
      "TRAIN [5,   410] loss: 0.616\n",
      "TRAIN [5,   415] loss: 0.627\n",
      "TRAIN [5,   420] loss: 0.609\n",
      "TRAIN [5,   425] loss: 0.647\n",
      "TRAIN [5,   430] loss: 0.582\n",
      "DEV [5,   433] loss: 0.462\n",
      "TRAIN [6,     5] loss: 0.703\n",
      "TRAIN [6,    10] loss: 0.762\n",
      "TRAIN [6,    15] loss: 0.691\n",
      "TRAIN [6,    20] loss: 0.625\n",
      "TRAIN [6,    25] loss: 0.575\n",
      "TRAIN [6,    30] loss: 0.680\n",
      "TRAIN [6,    35] loss: 0.655\n",
      "TRAIN [6,    40] loss: 0.528\n",
      "TRAIN [6,    45] loss: 0.566\n",
      "TRAIN [6,    50] loss: 0.595\n",
      "TRAIN [6,    55] loss: 0.603\n",
      "TRAIN [6,    60] loss: 0.661\n",
      "TRAIN [6,    65] loss: 0.481\n",
      "TRAIN [6,    70] loss: 0.570\n",
      "TRAIN [6,    75] loss: 0.567\n",
      "TRAIN [6,    80] loss: 0.602\n",
      "TRAIN [6,    85] loss: 0.505\n",
      "TRAIN [6,    90] loss: 0.560\n",
      "TRAIN [6,    95] loss: 0.622\n",
      "TRAIN [6,   100] loss: 0.561\n",
      "TRAIN [6,   105] loss: 0.658\n",
      "TRAIN [6,   110] loss: 0.596\n",
      "TRAIN [6,   115] loss: 0.492\n",
      "TRAIN [6,   120] loss: 0.492\n",
      "TRAIN [6,   125] loss: 0.542\n",
      "TRAIN [6,   130] loss: 0.493\n",
      "TRAIN [6,   135] loss: 0.501\n",
      "TRAIN [6,   140] loss: 0.561\n",
      "TRAIN [6,   145] loss: 0.506\n",
      "TRAIN [6,   150] loss: 0.468\n",
      "TRAIN [6,   155] loss: 0.621\n",
      "TRAIN [6,   160] loss: 0.532\n",
      "TRAIN [6,   165] loss: 0.525\n",
      "TRAIN [6,   170] loss: 0.453\n",
      "TRAIN [6,   175] loss: 0.535\n",
      "TRAIN [6,   180] loss: 0.518\n",
      "TRAIN [6,   185] loss: 0.507\n",
      "TRAIN [6,   190] loss: 0.508\n",
      "TRAIN [6,   195] loss: 0.467\n",
      "TRAIN [6,   200] loss: 0.477\n",
      "TRAIN [6,   205] loss: 0.512\n",
      "TRAIN [6,   210] loss: 0.488\n",
      "TRAIN [6,   215] loss: 0.522\n",
      "TRAIN [6,   220] loss: 0.479\n",
      "TRAIN [6,   225] loss: 0.556\n",
      "TRAIN [6,   230] loss: 0.552\n",
      "TRAIN [6,   235] loss: 0.530\n",
      "TRAIN [6,   240] loss: 0.492\n",
      "TRAIN [6,   245] loss: 0.504\n",
      "TRAIN [6,   250] loss: 0.458\n",
      "TRAIN [6,   255] loss: 0.470\n",
      "TRAIN [6,   260] loss: 0.442\n",
      "TRAIN [6,   265] loss: 0.502\n",
      "TRAIN [6,   270] loss: 0.522\n",
      "TRAIN [6,   275] loss: 0.497\n",
      "TRAIN [6,   280] loss: 0.458\n",
      "TRAIN [6,   285] loss: 0.524\n",
      "TRAIN [6,   290] loss: 0.542\n",
      "TRAIN [6,   295] loss: 0.510\n",
      "TRAIN [6,   300] loss: 0.476\n",
      "TRAIN [6,   305] loss: 0.485\n",
      "TRAIN [6,   310] loss: 0.441\n",
      "TRAIN [6,   315] loss: 0.547\n",
      "TRAIN [6,   320] loss: 0.553\n",
      "TRAIN [6,   325] loss: 0.483\n",
      "TRAIN [6,   330] loss: 0.521\n",
      "TRAIN [6,   335] loss: 0.422\n",
      "TRAIN [6,   340] loss: 0.415\n",
      "TRAIN [6,   345] loss: 0.591\n",
      "TRAIN [6,   350] loss: 0.459\n",
      "TRAIN [6,   355] loss: 0.481\n",
      "TRAIN [6,   360] loss: 0.508\n",
      "TRAIN [6,   365] loss: 0.484\n",
      "TRAIN [6,   370] loss: 0.367\n",
      "TRAIN [6,   375] loss: 0.424\n",
      "TRAIN [6,   380] loss: 0.468\n",
      "TRAIN [6,   385] loss: 0.477\n",
      "TRAIN [6,   390] loss: 0.460\n",
      "TRAIN [6,   395] loss: 0.470\n",
      "TRAIN [6,   400] loss: 0.468\n",
      "TRAIN [6,   405] loss: 0.376\n",
      "TRAIN [6,   410] loss: 0.477\n",
      "TRAIN [6,   415] loss: 0.407\n",
      "TRAIN [6,   420] loss: 0.507\n",
      "TRAIN [6,   425] loss: 0.407\n",
      "TRAIN [6,   430] loss: 0.451\n",
      "DEV [6,   433] loss: 0.246\n",
      "TRAIN [7,     5] loss: 0.452\n",
      "TRAIN [7,    10] loss: 0.465\n",
      "TRAIN [7,    15] loss: 0.383\n",
      "TRAIN [7,    20] loss: 0.469\n",
      "TRAIN [7,    25] loss: 0.362\n",
      "TRAIN [7,    30] loss: 0.377\n",
      "TRAIN [7,    35] loss: 0.341\n",
      "TRAIN [7,    40] loss: 0.431\n",
      "TRAIN [7,    45] loss: 0.472\n",
      "TRAIN [7,    50] loss: 0.437\n",
      "TRAIN [7,    55] loss: 0.430\n",
      "TRAIN [7,    60] loss: 0.397\n",
      "TRAIN [7,    65] loss: 0.380\n",
      "TRAIN [7,    70] loss: 0.367\n",
      "TRAIN [7,    75] loss: 0.397\n",
      "TRAIN [7,    80] loss: 0.346\n",
      "TRAIN [7,    85] loss: 0.426\n",
      "TRAIN [7,    90] loss: 0.389\n",
      "TRAIN [7,    95] loss: 0.400\n",
      "TRAIN [7,   100] loss: 0.305\n",
      "TRAIN [7,   105] loss: 0.278\n",
      "TRAIN [7,   110] loss: 0.391\n",
      "TRAIN [7,   115] loss: 0.330\n",
      "TRAIN [7,   120] loss: 0.282\n",
      "TRAIN [7,   125] loss: 0.384\n",
      "TRAIN [7,   130] loss: 0.429\n",
      "TRAIN [7,   135] loss: 0.369\n",
      "TRAIN [7,   140] loss: 0.346\n",
      "TRAIN [7,   145] loss: 0.369\n",
      "TRAIN [7,   150] loss: 0.513\n",
      "TRAIN [7,   155] loss: 0.393\n",
      "TRAIN [7,   160] loss: 0.391\n",
      "TRAIN [7,   165] loss: 0.343\n",
      "TRAIN [7,   170] loss: 0.432\n",
      "TRAIN [7,   175] loss: 0.313\n",
      "TRAIN [7,   180] loss: 0.355\n",
      "TRAIN [7,   185] loss: 0.383\n",
      "TRAIN [7,   190] loss: 0.322\n",
      "TRAIN [7,   195] loss: 0.349\n",
      "TRAIN [7,   200] loss: 0.355\n",
      "TRAIN [7,   205] loss: 0.284\n",
      "TRAIN [7,   210] loss: 0.369\n",
      "TRAIN [7,   215] loss: 0.364\n",
      "TRAIN [7,   220] loss: 0.384\n",
      "TRAIN [7,   225] loss: 0.398\n",
      "TRAIN [7,   230] loss: 0.281\n",
      "TRAIN [7,   235] loss: 0.319\n",
      "TRAIN [7,   240] loss: 0.341\n",
      "TRAIN [7,   245] loss: 0.263\n",
      "TRAIN [7,   250] loss: 0.312\n",
      "TRAIN [7,   255] loss: 0.397\n",
      "TRAIN [7,   260] loss: 0.301\n",
      "TRAIN [7,   265] loss: 0.350\n",
      "TRAIN [7,   270] loss: 0.351\n",
      "TRAIN [7,   275] loss: 0.340\n",
      "TRAIN [7,   280] loss: 0.338\n",
      "TRAIN [7,   285] loss: 0.289\n",
      "TRAIN [7,   290] loss: 0.294\n",
      "TRAIN [7,   295] loss: 0.374\n",
      "TRAIN [7,   300] loss: 0.363\n",
      "TRAIN [7,   305] loss: 0.269\n",
      "TRAIN [7,   310] loss: 0.281\n",
      "TRAIN [7,   315] loss: 0.400\n",
      "TRAIN [7,   320] loss: 0.264\n",
      "TRAIN [7,   325] loss: 0.275\n",
      "TRAIN [7,   330] loss: 0.376\n",
      "TRAIN [7,   335] loss: 0.313\n",
      "TRAIN [7,   340] loss: 0.254\n",
      "TRAIN [7,   345] loss: 0.291\n",
      "TRAIN [7,   350] loss: 0.264\n",
      "TRAIN [7,   355] loss: 0.326\n",
      "TRAIN [7,   360] loss: 0.223\n",
      "TRAIN [7,   365] loss: 0.306\n",
      "TRAIN [7,   370] loss: 0.271\n",
      "TRAIN [7,   375] loss: 0.250\n",
      "TRAIN [7,   380] loss: 0.300\n",
      "TRAIN [7,   385] loss: 0.329\n",
      "TRAIN [7,   390] loss: 0.312\n",
      "TRAIN [7,   395] loss: 0.338\n",
      "TRAIN [7,   400] loss: 0.254\n",
      "TRAIN [7,   405] loss: 0.241\n",
      "TRAIN [7,   410] loss: 0.321\n",
      "TRAIN [7,   415] loss: 0.237\n",
      "TRAIN [7,   420] loss: 0.294\n",
      "TRAIN [7,   425] loss: 0.369\n",
      "TRAIN [7,   430] loss: 0.332\n",
      "DEV [7,   433] loss: 0.133\n",
      "TRAIN [8,     5] loss: 0.370\n",
      "TRAIN [8,    10] loss: 0.274\n",
      "TRAIN [8,    15] loss: 0.283\n",
      "TRAIN [8,    20] loss: 0.366\n",
      "TRAIN [8,    25] loss: 0.362\n",
      "TRAIN [8,    30] loss: 0.247\n",
      "TRAIN [8,    35] loss: 0.243\n",
      "TRAIN [8,    40] loss: 0.271\n",
      "TRAIN [8,    45] loss: 0.263\n",
      "TRAIN [8,    50] loss: 0.330\n",
      "TRAIN [8,    55] loss: 0.293\n",
      "TRAIN [8,    60] loss: 0.314\n",
      "TRAIN [8,    65] loss: 0.253\n",
      "TRAIN [8,    70] loss: 0.255\n",
      "TRAIN [8,    75] loss: 0.216\n",
      "TRAIN [8,    80] loss: 0.363\n",
      "TRAIN [8,    85] loss: 0.253\n",
      "TRAIN [8,    90] loss: 0.209\n",
      "TRAIN [8,    95] loss: 0.202\n",
      "TRAIN [8,   100] loss: 0.248\n",
      "TRAIN [8,   105] loss: 0.268\n",
      "TRAIN [8,   110] loss: 0.210\n",
      "TRAIN [8,   115] loss: 0.317\n",
      "TRAIN [8,   120] loss: 0.207\n",
      "TRAIN [8,   125] loss: 0.223\n",
      "TRAIN [8,   130] loss: 0.259\n",
      "TRAIN [8,   135] loss: 0.256\n",
      "TRAIN [8,   140] loss: 0.254\n",
      "TRAIN [8,   145] loss: 0.189\n",
      "TRAIN [8,   150] loss: 0.241\n",
      "TRAIN [8,   155] loss: 0.197\n",
      "TRAIN [8,   160] loss: 0.259\n",
      "TRAIN [8,   165] loss: 0.235\n",
      "TRAIN [8,   170] loss: 0.306\n",
      "TRAIN [8,   175] loss: 0.261\n",
      "TRAIN [8,   180] loss: 0.204\n",
      "TRAIN [8,   185] loss: 0.213\n",
      "TRAIN [8,   190] loss: 0.276\n",
      "TRAIN [8,   195] loss: 0.230\n",
      "TRAIN [8,   200] loss: 0.229\n",
      "TRAIN [8,   205] loss: 0.210\n",
      "TRAIN [8,   210] loss: 0.196\n",
      "TRAIN [8,   215] loss: 0.248\n",
      "TRAIN [8,   220] loss: 0.253\n",
      "TRAIN [8,   225] loss: 0.193\n",
      "TRAIN [8,   230] loss: 0.232\n",
      "TRAIN [8,   235] loss: 0.319\n",
      "TRAIN [8,   240] loss: 0.247\n",
      "TRAIN [8,   245] loss: 0.192\n",
      "TRAIN [8,   250] loss: 0.308\n",
      "TRAIN [8,   255] loss: 0.188\n",
      "TRAIN [8,   260] loss: 0.226\n",
      "TRAIN [8,   265] loss: 0.281\n",
      "TRAIN [8,   270] loss: 0.267\n",
      "TRAIN [8,   275] loss: 0.204\n",
      "TRAIN [8,   280] loss: 0.198\n",
      "TRAIN [8,   285] loss: 0.205\n",
      "TRAIN [8,   290] loss: 0.188\n",
      "TRAIN [8,   295] loss: 0.223\n",
      "TRAIN [8,   300] loss: 0.212\n",
      "TRAIN [8,   305] loss: 0.231\n",
      "TRAIN [8,   310] loss: 0.221\n",
      "TRAIN [8,   315] loss: 0.236\n",
      "TRAIN [8,   320] loss: 0.170\n",
      "TRAIN [8,   325] loss: 0.199\n",
      "TRAIN [8,   330] loss: 0.222\n",
      "TRAIN [8,   335] loss: 0.220\n",
      "TRAIN [8,   340] loss: 0.173\n",
      "TRAIN [8,   345] loss: 0.234\n",
      "TRAIN [8,   350] loss: 0.199\n",
      "TRAIN [8,   355] loss: 0.186\n",
      "TRAIN [8,   360] loss: 0.192\n",
      "TRAIN [8,   365] loss: 0.210\n",
      "TRAIN [8,   370] loss: 0.210\n",
      "TRAIN [8,   375] loss: 0.197\n",
      "TRAIN [8,   380] loss: 0.221\n",
      "TRAIN [8,   385] loss: 0.204\n",
      "TRAIN [8,   390] loss: 0.149\n",
      "TRAIN [8,   395] loss: 0.208\n",
      "TRAIN [8,   400] loss: 0.167\n",
      "TRAIN [8,   405] loss: 0.225\n",
      "TRAIN [8,   410] loss: 0.179\n",
      "TRAIN [8,   415] loss: 0.172\n",
      "TRAIN [8,   420] loss: 0.216\n",
      "TRAIN [8,   425] loss: 0.200\n",
      "TRAIN [8,   430] loss: 0.165\n",
      "DEV [8,   433] loss: 0.090\n",
      "TRAIN [9,     5] loss: 0.169\n",
      "TRAIN [9,    10] loss: 0.209\n",
      "TRAIN [9,    15] loss: 0.239\n",
      "TRAIN [9,    20] loss: 0.171\n",
      "TRAIN [9,    25] loss: 0.193\n",
      "TRAIN [9,    30] loss: 0.232\n",
      "TRAIN [9,    35] loss: 0.193\n",
      "TRAIN [9,    40] loss: 0.175\n",
      "TRAIN [9,    45] loss: 0.190\n",
      "TRAIN [9,    50] loss: 0.189\n",
      "TRAIN [9,    55] loss: 0.236\n",
      "TRAIN [9,    60] loss: 0.194\n",
      "TRAIN [9,    65] loss: 0.255\n",
      "TRAIN [9,    70] loss: 0.220\n",
      "TRAIN [9,    75] loss: 0.176\n",
      "TRAIN [9,    80] loss: 0.200\n",
      "TRAIN [9,    85] loss: 0.201\n",
      "TRAIN [9,    90] loss: 0.175\n",
      "TRAIN [9,    95] loss: 0.238\n",
      "TRAIN [9,   100] loss: 0.225\n",
      "TRAIN [9,   105] loss: 0.160\n",
      "TRAIN [9,   110] loss: 0.240\n",
      "TRAIN [9,   115] loss: 0.137\n",
      "TRAIN [9,   120] loss: 0.265\n",
      "TRAIN [9,   125] loss: 0.152\n",
      "TRAIN [9,   130] loss: 0.175\n",
      "TRAIN [9,   135] loss: 0.183\n",
      "TRAIN [9,   140] loss: 0.203\n",
      "TRAIN [9,   145] loss: 0.154\n",
      "TRAIN [9,   150] loss: 0.211\n",
      "TRAIN [9,   155] loss: 0.210\n",
      "TRAIN [9,   160] loss: 0.262\n",
      "TRAIN [9,   165] loss: 0.194\n",
      "TRAIN [9,   170] loss: 0.177\n",
      "TRAIN [9,   175] loss: 0.178\n",
      "TRAIN [9,   180] loss: 0.195\n",
      "TRAIN [9,   185] loss: 0.164\n",
      "TRAIN [9,   190] loss: 0.164\n",
      "TRAIN [9,   195] loss: 0.162\n",
      "TRAIN [9,   200] loss: 0.262\n",
      "TRAIN [9,   205] loss: 0.171\n",
      "TRAIN [9,   210] loss: 0.170\n",
      "TRAIN [9,   215] loss: 0.171\n",
      "TRAIN [9,   220] loss: 0.208\n",
      "TRAIN [9,   225] loss: 0.204\n",
      "TRAIN [9,   230] loss: 0.184\n",
      "TRAIN [9,   235] loss: 0.143\n",
      "TRAIN [9,   240] loss: 0.118\n",
      "TRAIN [9,   245] loss: 0.191\n",
      "TRAIN [9,   250] loss: 0.196\n",
      "TRAIN [9,   255] loss: 0.225\n",
      "TRAIN [9,   260] loss: 0.157\n",
      "TRAIN [9,   265] loss: 0.149\n",
      "TRAIN [9,   270] loss: 0.182\n",
      "TRAIN [9,   275] loss: 0.182\n",
      "TRAIN [9,   280] loss: 0.182\n",
      "TRAIN [9,   285] loss: 0.205\n",
      "TRAIN [9,   290] loss: 0.165\n",
      "TRAIN [9,   295] loss: 0.173\n",
      "TRAIN [9,   300] loss: 0.176\n",
      "TRAIN [9,   305] loss: 0.157\n",
      "TRAIN [9,   310] loss: 0.129\n",
      "TRAIN [9,   315] loss: 0.171\n",
      "TRAIN [9,   320] loss: 0.098\n",
      "TRAIN [9,   325] loss: 0.231\n",
      "TRAIN [9,   330] loss: 0.127\n",
      "TRAIN [9,   335] loss: 0.134\n",
      "TRAIN [9,   340] loss: 0.147\n",
      "TRAIN [9,   345] loss: 0.154\n",
      "TRAIN [9,   350] loss: 0.124\n",
      "TRAIN [9,   355] loss: 0.136\n",
      "TRAIN [9,   360] loss: 0.204\n",
      "TRAIN [9,   365] loss: 0.119\n",
      "TRAIN [9,   370] loss: 0.142\n",
      "TRAIN [9,   375] loss: 0.100\n",
      "TRAIN [9,   380] loss: 0.118\n",
      "TRAIN [9,   385] loss: 0.183\n",
      "TRAIN [9,   390] loss: 0.165\n",
      "TRAIN [9,   395] loss: 0.128\n",
      "TRAIN [9,   400] loss: 0.239\n",
      "TRAIN [9,   405] loss: 0.168\n",
      "TRAIN [9,   410] loss: 0.133\n",
      "TRAIN [9,   415] loss: 0.176\n",
      "TRAIN [9,   420] loss: 0.154\n",
      "TRAIN [9,   425] loss: 0.139\n",
      "TRAIN [9,   430] loss: 0.129\n",
      "DEV [9,   433] loss: 0.057\n",
      "TRAIN [10,     5] loss: 0.152\n",
      "TRAIN [10,    10] loss: 0.209\n",
      "TRAIN [10,    15] loss: 0.211\n",
      "TRAIN [10,    20] loss: 0.134\n",
      "TRAIN [10,    25] loss: 0.165\n",
      "TRAIN [10,    30] loss: 0.150\n",
      "TRAIN [10,    35] loss: 0.144\n",
      "TRAIN [10,    40] loss: 0.202\n",
      "TRAIN [10,    45] loss: 0.154\n",
      "TRAIN [10,    50] loss: 0.177\n",
      "TRAIN [10,    55] loss: 0.207\n",
      "TRAIN [10,    60] loss: 0.106\n",
      "TRAIN [10,    65] loss: 0.158\n",
      "TRAIN [10,    70] loss: 0.138\n",
      "TRAIN [10,    75] loss: 0.148\n",
      "TRAIN [10,    80] loss: 0.162\n",
      "TRAIN [10,    85] loss: 0.106\n",
      "TRAIN [10,    90] loss: 0.127\n",
      "TRAIN [10,    95] loss: 0.115\n",
      "TRAIN [10,   100] loss: 0.146\n",
      "TRAIN [10,   105] loss: 0.103\n",
      "TRAIN [10,   110] loss: 0.102\n",
      "TRAIN [10,   115] loss: 0.142\n",
      "TRAIN [10,   120] loss: 0.178\n",
      "TRAIN [10,   125] loss: 0.169\n",
      "TRAIN [10,   130] loss: 0.177\n",
      "TRAIN [10,   135] loss: 0.091\n",
      "TRAIN [10,   140] loss: 0.149\n",
      "TRAIN [10,   145] loss: 0.183\n",
      "TRAIN [10,   150] loss: 0.116\n",
      "TRAIN [10,   155] loss: 0.126\n",
      "TRAIN [10,   160] loss: 0.128\n",
      "TRAIN [10,   165] loss: 0.094\n",
      "TRAIN [10,   170] loss: 0.160\n",
      "TRAIN [10,   175] loss: 0.106\n",
      "TRAIN [10,   180] loss: 0.133\n",
      "TRAIN [10,   185] loss: 0.181\n",
      "TRAIN [10,   190] loss: 0.140\n",
      "TRAIN [10,   195] loss: 0.096\n",
      "TRAIN [10,   200] loss: 0.163\n",
      "TRAIN [10,   205] loss: 0.129\n",
      "TRAIN [10,   210] loss: 0.093\n",
      "TRAIN [10,   215] loss: 0.078\n",
      "TRAIN [10,   220] loss: 0.124\n",
      "TRAIN [10,   225] loss: 0.150\n",
      "TRAIN [10,   230] loss: 0.144\n",
      "TRAIN [10,   235] loss: 0.123\n",
      "TRAIN [10,   240] loss: 0.112\n",
      "TRAIN [10,   245] loss: 0.096\n",
      "TRAIN [10,   250] loss: 0.100\n",
      "TRAIN [10,   255] loss: 0.136\n",
      "TRAIN [10,   260] loss: 0.137\n",
      "TRAIN [10,   265] loss: 0.139\n",
      "TRAIN [10,   270] loss: 0.107\n",
      "TRAIN [10,   275] loss: 0.125\n",
      "TRAIN [10,   280] loss: 0.128\n",
      "TRAIN [10,   285] loss: 0.128\n",
      "TRAIN [10,   290] loss: 0.131\n",
      "TRAIN [10,   295] loss: 0.077\n",
      "TRAIN [10,   300] loss: 0.162\n",
      "TRAIN [10,   305] loss: 0.110\n",
      "TRAIN [10,   310] loss: 0.087\n",
      "TRAIN [10,   315] loss: 0.142\n",
      "TRAIN [10,   320] loss: 0.150\n",
      "TRAIN [10,   325] loss: 0.154\n",
      "TRAIN [10,   330] loss: 0.090\n",
      "TRAIN [10,   335] loss: 0.142\n",
      "TRAIN [10,   340] loss: 0.106\n",
      "TRAIN [10,   345] loss: 0.108\n",
      "TRAIN [10,   350] loss: 0.154\n",
      "TRAIN [10,   355] loss: 0.092\n",
      "TRAIN [10,   360] loss: 0.136\n",
      "TRAIN [10,   365] loss: 0.126\n",
      "TRAIN [10,   370] loss: 0.112\n",
      "TRAIN [10,   375] loss: 0.147\n",
      "TRAIN [10,   380] loss: 0.128\n",
      "TRAIN [10,   385] loss: 0.146\n",
      "TRAIN [10,   390] loss: 0.159\n",
      "TRAIN [10,   395] loss: 0.163\n",
      "TRAIN [10,   400] loss: 0.150\n",
      "TRAIN [10,   405] loss: 0.127\n",
      "TRAIN [10,   410] loss: 0.116\n",
      "TRAIN [10,   415] loss: 0.158\n",
      "TRAIN [10,   420] loss: 0.102\n",
      "TRAIN [10,   425] loss: 0.090\n",
      "TRAIN [10,   430] loss: 0.077\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ebdbc5c25d1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0madapters_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mcls_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./pretraining_saved_model_adaps.pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;32m~/Projects/RIET/DynamicQuery/src/dynamicquery/cross_query/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, device, train_dataloader, dev_dataloader, epochs, print_steps, adapters_only, cls_train, save_path)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minpt_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/RIET/DynamicQuery/src/dynamicquery/cross_query/extended_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, extended_states, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m         )\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/RIET/DynamicQuery/src/dynamicquery/cross_query/extended_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, extended_states, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m         )\n\u001b[1;32m    401\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/RIET/DynamicQuery/src/dynamicquery/cross_query/extended_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, extended_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    236\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m                 )\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/RIET/DynamicQuery/src/dynamicquery/cross_query/extended_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, extended_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         )\n\u001b[1;32m    122\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/RIET/DynamicQuery/src/dynamicquery/cross_query/extended_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, extended_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         )\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextension_prop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/RIET/DynamicQuery/src/dynamicquery/cross_query/extended_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, extension_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mextension_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_extensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         outputs = super().forward(\n\u001b[1;32m     29\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 15\n",
    "train.train(\n",
    "    model, \n",
    "    optimizer, \n",
    "    device,\n",
    "    train_dl,\n",
    "    dev_dl,\n",
    "    epochs=EPOCHS,\n",
    "    print_steps=5,\n",
    "    adapters_only=True, \n",
    "    cls_train=True,\n",
    "    save_path=\"./pretraining_saved_model_adaps.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "159c8c7d-ac98-4fe3-a00c-aff7a7e9c106",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(base_path, \"src/dynamicquery/cross_query/pretrain_model_10ep.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49dcd7cf-323c-4ba5-8989-06eb760cb1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2e9ba2a-15a6-4936-bad7-d66b5e286c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN [1,     5] loss: 1.621\n",
      "TRAIN [1,    10] loss: 1.728\n",
      "TRAIN [1,    15] loss: 1.715\n",
      "TRAIN [1,    20] loss: 1.714\n",
      "TRAIN [1,    25] loss: 1.688\n",
      "TRAIN [1,    30] loss: 1.617\n",
      "DEV [1,     7] loss: 1.831\n",
      "TRAIN [2,     5] loss: 1.549\n",
      "TRAIN [2,    10] loss: 1.360\n",
      "TRAIN [2,    15] loss: 1.369\n",
      "TRAIN [2,    20] loss: 1.268\n",
      "TRAIN [2,    25] loss: 1.354\n",
      "TRAIN [2,    30] loss: 1.333\n",
      "DEV [2,     7] loss: 2.165\n",
      "TRAIN [3,     5] loss: 1.127\n",
      "TRAIN [3,    10] loss: 1.445\n",
      "TRAIN [3,    15] loss: 1.388\n",
      "TRAIN [3,    20] loss: 1.381\n",
      "TRAIN [3,    25] loss: 1.115\n",
      "TRAIN [3,    30] loss: 1.270\n",
      "DEV [3,     7] loss: 1.899\n",
      "TRAIN [4,     5] loss: 1.196\n",
      "TRAIN [4,    10] loss: 1.154\n",
      "TRAIN [4,    15] loss: 1.164\n",
      "TRAIN [4,    20] loss: 1.131\n",
      "TRAIN [4,    25] loss: 1.118\n",
      "TRAIN [4,    30] loss: 1.100\n",
      "DEV [4,     7] loss: 1.844\n",
      "TRAIN [5,     5] loss: 1.384\n",
      "TRAIN [5,    10] loss: 1.442\n",
      "TRAIN [5,    15] loss: 1.341\n",
      "TRAIN [5,    20] loss: 1.253\n",
      "TRAIN [5,    25] loss: 1.075\n",
      "TRAIN [5,    30] loss: 1.093\n",
      "DEV [5,     7] loss: 2.085\n",
      "TRAIN [6,     5] loss: 1.088\n",
      "TRAIN [6,    10] loss: 1.040\n",
      "TRAIN [6,    15] loss: 1.039\n",
      "TRAIN [6,    20] loss: 1.082\n",
      "TRAIN [6,    25] loss: 1.048\n",
      "TRAIN [6,    30] loss: 1.005\n",
      "DEV [6,     7] loss: 2.369\n",
      "TRAIN [7,     5] loss: 0.936\n",
      "TRAIN [7,    10] loss: 0.880\n",
      "TRAIN [7,    15] loss: 0.986\n",
      "TRAIN [7,    20] loss: 1.006\n",
      "TRAIN [7,    25] loss: 0.961\n",
      "TRAIN [7,    30] loss: 0.931\n",
      "DEV [7,     7] loss: 2.294\n",
      "TRAIN [8,     5] loss: 0.922\n",
      "TRAIN [8,    10] loss: 0.838\n",
      "TRAIN [8,    15] loss: 0.845\n",
      "TRAIN [8,    20] loss: 0.794\n",
      "TRAIN [8,    25] loss: 0.983\n",
      "TRAIN [8,    30] loss: 0.846\n",
      "DEV [8,     7] loss: 2.014\n",
      "TRAIN [9,     5] loss: 0.798\n",
      "TRAIN [9,    10] loss: 0.921\n",
      "TRAIN [9,    15] loss: 0.825\n",
      "TRAIN [9,    20] loss: 0.705\n",
      "TRAIN [9,    25] loss: 0.780\n",
      "TRAIN [9,    30] loss: 0.743\n",
      "DEV [9,     7] loss: 2.356\n",
      "TRAIN [10,     5] loss: 0.735\n",
      "TRAIN [10,    10] loss: 0.767\n",
      "TRAIN [10,    15] loss: 0.752\n",
      "TRAIN [10,    20] loss: 1.014\n",
      "TRAIN [10,    25] loss: 0.921\n",
      "TRAIN [10,    30] loss: 0.894\n",
      "DEV [10,     7] loss: 2.020\n",
      "TRAIN [11,     5] loss: 0.833\n",
      "TRAIN [11,    10] loss: 0.799\n",
      "TRAIN [11,    15] loss: 0.818\n",
      "TRAIN [11,    20] loss: 0.685\n",
      "TRAIN [11,    25] loss: 0.725\n",
      "TRAIN [11,    30] loss: 0.735\n",
      "DEV [11,     7] loss: 2.200\n",
      "TRAIN [12,     5] loss: 0.685\n",
      "TRAIN [12,    10] loss: 0.654\n",
      "TRAIN [12,    15] loss: 0.547\n",
      "TRAIN [12,    20] loss: 0.677\n",
      "TRAIN [12,    25] loss: 0.878\n",
      "TRAIN [12,    30] loss: 0.645\n",
      "DEV [12,     7] loss: 2.201\n",
      "TRAIN [13,     5] loss: 0.767\n",
      "TRAIN [13,    10] loss: 0.749\n",
      "TRAIN [13,    15] loss: 0.648\n",
      "TRAIN [13,    20] loss: 0.572\n",
      "TRAIN [13,    25] loss: 0.507\n",
      "TRAIN [13,    30] loss: 0.697\n",
      "DEV [13,     7] loss: 2.646\n",
      "TRAIN [14,     5] loss: 0.537\n",
      "TRAIN [14,    10] loss: 0.463\n",
      "TRAIN [14,    15] loss: 0.535\n",
      "TRAIN [14,    20] loss: 0.489\n",
      "TRAIN [14,    25] loss: 0.540\n",
      "TRAIN [14,    30] loss: 0.581\n",
      "DEV [14,     7] loss: 2.550\n",
      "TRAIN [15,     5] loss: 0.420\n",
      "TRAIN [15,    10] loss: 0.447\n",
      "TRAIN [15,    15] loss: 0.446\n",
      "TRAIN [15,    20] loss: 0.455\n",
      "TRAIN [15,    25] loss: 0.418\n",
      "TRAIN [15,    30] loss: 0.466\n",
      "DEV [15,     7] loss: 2.854\n",
      "TRAIN [16,     5] loss: 0.377\n",
      "TRAIN [16,    10] loss: 0.334\n",
      "TRAIN [16,    15] loss: 0.425\n",
      "TRAIN [16,    20] loss: 0.457\n",
      "TRAIN [16,    25] loss: 0.396\n",
      "TRAIN [16,    30] loss: 0.343\n",
      "DEV [16,     7] loss: 3.255\n",
      "TRAIN [17,     5] loss: 0.254\n",
      "TRAIN [17,    10] loss: 0.348\n",
      "TRAIN [17,    15] loss: 0.285\n",
      "TRAIN [17,    20] loss: 0.262\n",
      "TRAIN [17,    25] loss: 0.417\n",
      "TRAIN [17,    30] loss: 0.264\n",
      "DEV [17,     7] loss: 3.318\n",
      "TRAIN [18,     5] loss: 0.223\n",
      "TRAIN [18,    10] loss: 0.288\n",
      "TRAIN [18,    15] loss: 0.277\n",
      "TRAIN [18,    20] loss: 0.210\n",
      "TRAIN [18,    25] loss: 0.229\n",
      "TRAIN [18,    30] loss: 0.318\n",
      "DEV [18,     7] loss: 3.540\n",
      "TRAIN [19,     5] loss: 0.244\n",
      "TRAIN [19,    10] loss: 0.219\n",
      "TRAIN [19,    15] loss: 0.202\n",
      "TRAIN [19,    20] loss: 0.205\n",
      "TRAIN [19,    25] loss: 0.203\n",
      "TRAIN [19,    30] loss: 0.228\n",
      "DEV [19,     7] loss: 3.253\n",
      "TRAIN [20,     5] loss: 0.306\n",
      "TRAIN [20,    10] loss: 0.381\n",
      "TRAIN [20,    15] loss: 0.275\n",
      "TRAIN [20,    20] loss: 0.292\n",
      "TRAIN [20,    25] loss: 0.292\n",
      "TRAIN [20,    30] loss: 0.256\n",
      "DEV [20,     7] loss: 3.257\n",
      "TRAIN [21,     5] loss: 0.198\n",
      "TRAIN [21,    10] loss: 0.196\n",
      "TRAIN [21,    15] loss: 0.227\n",
      "TRAIN [21,    20] loss: 0.193\n",
      "TRAIN [21,    25] loss: 0.177\n",
      "TRAIN [21,    30] loss: 0.140\n",
      "DEV [21,     7] loss: 3.836\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "train.train(\n",
    "    model, \n",
    "    optimizer, \n",
    "    device,\n",
    "    train_dl,\n",
    "    dev_dl,\n",
    "    epochs=EPOCHS+1,\n",
    "    print_steps=5,\n",
    "    adapters_only=False, \n",
    "    cls_train=True,\n",
    "    save_path=\"./cross_query/test_saved_model_5.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c75acfd7-f7a4-4f36-87f6-7810b6522006",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./cross_query/test_saved_model_5.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d69e44f5-95a8-4a4b-94df-f3601dae1e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.zeros((2,2))\n",
    "x2 = torch.ones((2,2))\n",
    "\n",
    "torch.save(x1, \"temp-delete.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28af3ce7-2db1-4994-b59a-364027721739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.load(\"temp-delete.pt\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "785c87f8-7699-43e3-ae8c-52634facbd57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(x2, \"temp-delete.pt\")\n",
    "x = torch.load(\"temp-delete.pt\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86dbeb0b-212c-43c1-bae9-37aed3c64a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_load_state_dict_into_model',\n",
       " '_load_state_dict_into_model_low_mem',\n",
       " '_prepare_model_inputs',\n",
       " '_update_model_kwargs_for_generation',\n",
       " 'base_model',\n",
       " 'base_model_prefix']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda u: \"mode\" in u.lower(), dir(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ad6283-2f42-4551-a038-a846066970b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
