{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "653f0986-baec-456f-a937-41fb25bcc2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_path = \"/home/mshlis/Projects/RIET/DynamicQuery\"\n",
    "os.chdir(base_path)\n",
    "\n",
    "from dynamicquery import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7093d89-7d06-4a5b-92f2-c3d50385e3f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Clef2021RerankedDataset(TensorDataset):\n",
    "    def __init__(self, \n",
    "                 encode_fn, \n",
    "                 claims, \n",
    "                 tweets, \n",
    "                 connections,\n",
    "                 claim_embeddings,\n",
    "                 ranks):\n",
    "        self.claim_embeddings = claim_embeddings\n",
    "        self.ranks = ranks\n",
    "        run_tweets = tweets.join(connections.set_index(\"tweet_id\"), on=\"id\", how=\"inner\")\n",
    "        run_tweets = run_tweets.join(claims.set_index(\"vclaim_id\"), on=\"claim_id\", how=\"inner\")\n",
    "        run_tweets = run_tweets[[\"tweet\", \"vclaim\"]].reset_index()\n",
    "        run_tweets[\"encoded_tweet\"] = run_tweets.tweet.apply(encode_fn)\n",
    "        self.claim_idx = [claims.vclaim.to_list().index(t_claim) for t_claim in run_tweets.vclaim.to_list()]\n",
    "        self.data = run_tweets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        Xt = self.data.encoded_tweet[index]\n",
    "        Xt = (np.array(Xt[\"input_ids\"]), np.array(Xt[\"attention_mask\"]))\n",
    "        Xe = self.claim_embeddings[self.ranks[index]]\n",
    "        # Ye = self.claim_embeddings[self.claim_idx[index:index+1]]\n",
    "        return (Xt, Xe)\n",
    "    \n",
    "    \n",
    "def get_clef2021_reranked_dataloader(encode_fn, \n",
    "                            claims, \n",
    "                            tweets, \n",
    "                            connections, \n",
    "                            claim_embeddings,\n",
    "                            ranks,\n",
    "                            params={'batch_size':32, 'shuffle':True}):\n",
    "    dataset = Clef2021RerankedDataset(encode_fn, \n",
    "                              claims, \n",
    "                              tweets, \n",
    "                              connections, \n",
    "                              claim_embeddings,\n",
    "                              ranks)\n",
    "    return DataLoader(dataset, **params)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e609675-bbc1-43a4-a44a-3c2aef866d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_path = \"./experiments/candidate_selection/finetune_st5_large_claims_negs\"\n",
    "train_neg_path = os.path.join(exp_path, \"ranks_train.npy\")\n",
    "dev_neg_path = os.path.join(exp_path, \"ranks_dev.npy\")\n",
    "emb_path = os.path.join(exp_path, \"claim_embs.npy\")\n",
    "tweet_emb_path = os.path.join(exp_path, \"tweet_embs.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5af9377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/mshlis/Projects/RIET/DynamicQuery', '4.17.0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "os.getcwd(), transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb702fa9-ef41-4f68-b09d-ef839e2afa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_ids = np.load(train_neg_path)\n",
    "dev_neg_ids = np.load(dev_neg_path)\n",
    "neg_embs = np.load(emb_path)\n",
    "tweet_embs = np.load(tweet_emb_path, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4b7c0a6-1d01-41a2-8e71-97dabce4336a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((999, 13825), (13825, 768))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_ids.shape, neg_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92395a0b-6e95-4737-a468-48160b480a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing ExtendedRobertaForExternalClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing ExtendedRobertaForExternalClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ExtendedRobertaForExternalClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ExtendedRobertaForExternalClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.adapter_layer.bias', 'classifier.weight', 'roberta.adapter_layer.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import extended_roberta_v2 as roberta\n",
    "from transformers import AutoTokenizer\n",
    "from functools import partial\n",
    "import importlib\n",
    "importlib.reload(roberta)\n",
    "\n",
    "MAX_LENGTH = 192\n",
    "\n",
    "model_str = \"roberta-base\"\n",
    "model = roberta.ExtendedRobertaForExternalClassification.from_pretrained(model_str)\n",
    "model.load_state_dict(torch.load(\"./experiments/cross_query/base_rndm_large_neg_v2/trained_model.pt\"))\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_str)\n",
    "tokenize = partial(tokenizer, **dict(\n",
    "    truncation=True, \n",
    "    max_length=MAX_LENGTH, \n",
    "    padding=\"max_length\", \n",
    "    return_attention_mask=True\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4d660d6-f41b-4867-81bb-676f6fe2e894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.adapter_layer\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def is_adapter(name): \n",
    "    check_v1 = re.match(\"roberta.encoder\\.adapter_layer\\.\", name)\n",
    "    check_v2 = name == \"roberta.adapter_layer\"\n",
    "    return check_v1 or check_v2\n",
    "\n",
    "for name, param in model.named_modules():\n",
    "    if is_adapter(name):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffcaf466-8b8a-4fb8-83af-d11a44f6d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claim Data\n",
    "tweets, test_tweets = utils.get_tweets()\n",
    "test_tweets = test_tweets[1:]\n",
    "train_conns, dev_conns, test_conns = utils.get_qrels()\n",
    "claims = utils.get_claims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0332a064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (dev_neg_ids[:, 0] == claim_idx[:]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "817212db-4b4a-42b9-a577-45f85696b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataloaders\n",
    "importlib.reload(dataloaders)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dl = dataloaders.get_clef2021_reranked_eval_dataloader(\n",
    "    tokenize, \n",
    "    claims, \n",
    "    tweets, \n",
    "    train_conns,\n",
    "    neg_embs,\n",
    "    neg_ids[:,:5],\n",
    "    tweet_embs[()][\"train\"],\n",
    "    params={'batch_size':BATCH_SIZE, 'shuffle':False})\n",
    "\n",
    "dev_dl = dataloaders.get_clef2021_reranked_eval_dataloader(\n",
    "    tokenize, \n",
    "    claims, \n",
    "    tweets, \n",
    "    dev_conns,\n",
    "    neg_embs,\n",
    "    dev_neg_ids[:,:5],\n",
    "    tweet_embs[()][\"dev\"],\n",
    "    params={'batch_size':BATCH_SIZE, 'shuffle':False}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96276fc9-7588-4791-a3c5-a2441c13c45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "probits = []\n",
    "for inputs, external_inputs in train_dl:\n",
    "    inpt_dict = {\n",
    "        \"input_ids\": inputs[0],\n",
    "        \"attention_mask\": inputs[1],\n",
    "        \"extended_states\": external_inputs,\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        out = model(**inpt_dict)\n",
    "        _probits = torch.nn.functional.softmax(out.logits[:,:-1], dim=-1)\n",
    "    probits.append(_probits.detach().numpy())\n",
    "    \n",
    "probits = np.concatenate(probits, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22cf7f0d-b7e3-430b-a869-ee4ecd07e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_probits = []\n",
    "for inputs, external_inputs in dev_dl:\n",
    "    inpt_dict = {\n",
    "        \"input_ids\": inputs[0],\n",
    "        \"attention_mask\": inputs[1],\n",
    "        \"extended_states\": external_inputs\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        out = model(**inpt_dict)\n",
    "        _probits = torch.nn.functional.softmax(out.logits[:,:-1], dim=-1)\n",
    "    dev_probits.append(_probits.detach().numpy())\n",
    "    \n",
    "dev_probits = np.concatenate(dev_probits, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8738cce8-bb2d-46fa-b487-a826cfbbad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranks = probits.argsort()[:,::-1]\n",
    "dev_reranks = dev_probits.argsort()[:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da5c2b69-761b-4482-b334-4845c1dc8357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e6f3db0-6671-497d-a624-0f609e420a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4 0]\n",
      " [0 4 1 3 2]\n",
      " [0 2 4 3 1]\n",
      " [0 2 1 4 3]\n",
      " [3 0 4 1 2]]\n",
      "[[2 0 3 1 4]\n",
      " [0 4 1 2 3]\n",
      " [0 1 4 2 3]\n",
      " [4 0 1 2 3]\n",
      " [0 2 1 3 4]]\n"
     ]
    }
   ],
   "source": [
    "print(reranks[:5])\n",
    "print(dev_reranks[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a942f91f-f3fd-4ffb-b2d9-9a9a0a9e82b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4 0]\n",
      " [0 4 1 3 2]\n",
      " [0 2 4 3 1]\n",
      " [0 2 1 4 3]\n",
      " [3 0 4 1 2]]\n",
      "[[2 0 3 1 4]\n",
      " [0 4 1 2 3]\n",
      " [0 1 4 2 3]\n",
      " [4 0 1 2 3]\n",
      " [0 2 1 3 4]]\n"
     ]
    }
   ],
   "source": [
    "print(reranks[:5])\n",
    "print(dev_reranks[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e58d311-5afc-468e-a1e6-226260bc4f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx(connections, claims, tweets):\n",
    "    run_tweets = tweets.join(connections.set_index(\"tweet_id\"), on=\"id\", how=\"inner\")\n",
    "    run_tweets = run_tweets.join(claims.set_index(\"vclaim_id\"), on=\"claim_id\", how=\"inner\")\n",
    "    run_tweets = run_tweets[[\"tweet\", \"vclaim\"]].reset_index()\n",
    "    claim_idx = [claims.vclaim.to_list().index(t_claim) for t_claim in run_tweets.vclaim.to_list()]\n",
    "    return run_tweets, claim_idx\n",
    "\n",
    "def avg_prec(gold, rankings, n):\n",
    "    is_rel = (np.array(rankings)[:n] == gold).astype(float)\n",
    "    return (is_rel/np.arange(1,n+1)).sum()\n",
    "\n",
    "def recall(gold, rankings, n):\n",
    "    is_rel = (np.array(rankings)[:n] == gold).astype(float)\n",
    "    return is_rel.sum()\n",
    "\n",
    "def mean_avg_prec(golds, rankings, n):\n",
    "    avg_precs = [avg_prec(gold, rlist, n) for gold, rlist in zip(golds, rankings)]\n",
    "    return np.array(avg_precs).mean()\n",
    "\n",
    "def mean_recall(golds, rankings, n):\n",
    "    avg_precs = [recall(gold, rlist, n) for gold, rlist in zip(golds, rankings)]\n",
    "    return np.array(avg_precs).mean()\n",
    "\n",
    "def get_negative_ranks(ranks, gold):\n",
    "    return [r for r in ranks if r!=gold]\n",
    "\n",
    "def get_negative_ranks_arr(ranks, gold):\n",
    "    n_ranks = [get_negative_ranks(r, g) for r,g in zip(ranks, claim_idx)]\n",
    "    return np.array(n_ranks)\n",
    "\n",
    "map_results = {}\n",
    "map_recall_results = {}\n",
    "save_ranks = False\n",
    "for ptn in [\"train\", \"dev\"]:\n",
    "    if ptn == \"train\":\n",
    "        run_tweets, claim_idx = get_idx(train_conns, claims, tweets)\n",
    "        ranks = np.array([ids[rerank] for ids, rerank in zip(neg_ids, reranks)])\n",
    "    elif ptn == \"dev\":\n",
    "        run_tweets, claim_idx = get_idx(dev_conns, claims, tweets)\n",
    "        ranks = np.array([ids[rerank] for ids, rerank in zip(dev_neg_ids, dev_reranks)])\n",
    "    elif ptn == \"test\":\n",
    "        run_tweets, claim_idx = get_idx(test_conns, claims, test_tweets)\n",
    "\n",
    "    if save_ranks:\n",
    "        np.save(f\"./experiments/finetune_st5_large_claims_negs/negative_embs_{ptn}.npy\",\n",
    "                get_negative_ranks_arr(ranks, claim_idx))\n",
    "        np.save(f\"./experiments/finetune_st5_large_claims_negs/ranks_{ptn}.npy\",\n",
    "                np.array(ranks))\n",
    "    \n",
    "    map_results[ptn] = []\n",
    "    for n in [1,5]:\n",
    "        map_results[ptn].append(mean_avg_prec(claim_idx, ranks, n))\n",
    "        \n",
    "    map_recall_results[ptn] = []\n",
    "    for n in [1,5]:\n",
    "        map_recall_results[ptn].append(mean_recall(claim_idx, ranks, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24c3194f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1184,  9221, 13600,  2843,  6580],\n",
       "       [ 9221,  4757,  5270,  3998,  3767],\n",
       "       [ 7808, 12947,  4065,  6213,  6447],\n",
       "       [ 4388,  1640,  5398, 13760,  8051],\n",
       "       [ 1640, 13760,  6823,  5309,  8051],\n",
       "       [11060, 10734,   874,  6134, 10195],\n",
       "       [ 9380, 12609, 13229,  9611,   456],\n",
       "       [ 3926, 10537,  1658,  6830,  4077],\n",
       "       [ 9764,  1721,  9655,  6676,  1476],\n",
       "       [  177,  1476,  9764,  9655,  3812]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4360f01f-b1c1-45d1-8de1-538a5d4c969a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': [0.6916916916916916, 0.8087921254587921],\n",
       " 'dev': [0.44, 0.6563333333333332]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73ad51f6-0501-4fe3-97d2-e11d3e8ad512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': [0.6916916916916916, 0.9629629629629629], 'dev': [0.44, 0.975]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_recall_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244edc21-2702-409f-807d-b6c4d83dfde2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035b6225-d1bd-4d2f-be22-1955d7d44819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee79c33a-ed64-4a45-bef8-2ec2b91f5400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a84b5b6-ac97-4d94-8816-cf0880e10e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cb2d17-3628-4b30-a33c-dc0255550334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb34ebb-054e-4475-9d77-bb48d5d36f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7daad4f-7d9c-4c5a-afaa-bfe9798c905e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".063 * len(dev_dl.dataset) / len(dev_dl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0f273a3-363c-4daa-beac-7e09203329c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfa1f75f-11ae-48da-ab4e-dbbd028dd365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.283333333333333"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns = list(range(1,6))\n",
    "sum([1/n for n in ns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa01a958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5919c20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.43977, 0.44525]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mult = sum([1/n for n in ns]) / 5\n",
    "vals = [.963, .975]\n",
    "[mult*v for v in vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9d2e149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "m = 200\n",
    "A = np.stack([np.random.permutation(k) for _ in range(m)], 0)\n",
    "bA = (A == 0).astype(int)\n",
    "print(bA[:,0].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8010a5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 3, 4])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54c92b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
