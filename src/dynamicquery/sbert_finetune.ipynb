{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76b30f04-91fa-499e-8503-ba9553d866ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06c97dbf-388e-42a2-b833-e37004889e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets, test_tweets = utils.get_tweets()\n",
    "test_tweets = test_tweets[1:]\n",
    "train_conns, dev_conns, test_conns = utils.get_qrels()\n",
    "claims = utils.get_claims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8615d95-ff4e-4c21-83ea-e198e4960ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "model = SentenceTransformer(\"sentence-transformers/sentence-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b65c373a-43bd-4b1a-83e2-effece0073bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "tokenize = partial(model.tokenizer, **dict(\n",
    "    truncation=True, \n",
    "    max_length=MAX_LENGTH, \n",
    "    padding=\"max_length\", \n",
    "    return_attention_mask=True\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8e0b463-7cd6-45b9-ab8a-fbdd1ae6be56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataloaders\n",
    "import importlib\n",
    "importlib.reload(dataloaders)\n",
    "\n",
    "train_dl = dataloaders.get_clef2021_dataloader(tokenize, claims, tweets, train_conns, \n",
    "                                               {'batch_size':BATCH_SIZE, 'shuffle':True})    \n",
    "dev_dl = dataloaders.get_clef2021_dataloader(tokenize, claims, tweets, dev_conns, \n",
    "                                               {'batch_size':BATCH_SIZE, 'shuffle':False})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32fb374a-25dd-45a5-a1e9-62c4a8f027cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "nn = torch.nn\n",
    "\n",
    "LR = 1e-5\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5514cc4-58c2-40eb-be5e-1500542dd7dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a7aa3fe-3394-430f-9b42-077024eb3da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CE = nn.CrossEntropyLoss()\n",
    "temp = .05\n",
    "def MNR_loss(left_tensors, right_tensors, negatives=None):\n",
    "    logits = torch.einsum(\"bd,cd->bc\", left_tensors, right_tensors)\n",
    "    return CE(logits / temp, torch.arange(logits.shape[0]).to(device))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3945d3bb-cf65-4240-9bd9-58194a0e1490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dl.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cbad12b-e899-4188-b131-15026a3dee18",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN [1,     5] loss: 0.893\n",
      "TRAIN [1,    10] loss: 0.776\n",
      "TRAIN [1,    15] loss: 0.688\n",
      "TRAIN [1,    20] loss: 0.657\n",
      "TRAIN [1,    25] loss: 0.422\n",
      "TRAIN [1,    30] loss: 0.448\n",
      "TRAIN [1,    35] loss: 0.389\n",
      "TRAIN [1,    40] loss: 0.354\n",
      "TRAIN [1,    45] loss: 0.317\n",
      "TRAIN [1,    50] loss: 0.297\n",
      "TRAIN [1,    55] loss: 0.277\n",
      "TRAIN [1,    60] loss: 0.298\n",
      "DEV [1,     5] loss: 0.580\n",
      "DEV [1,    10] loss: 0.447\n",
      "TRAIN [2,     5] loss: 0.239\n",
      "TRAIN [2,    10] loss: 0.210\n",
      "TRAIN [2,    15] loss: 0.181\n",
      "TRAIN [2,    20] loss: 0.167\n",
      "TRAIN [2,    25] loss: 0.231\n",
      "TRAIN [2,    30] loss: 0.155\n",
      "TRAIN [2,    35] loss: 0.125\n",
      "TRAIN [2,    40] loss: 0.140\n",
      "TRAIN [2,    45] loss: 0.166\n",
      "TRAIN [2,    50] loss: 0.128\n",
      "TRAIN [2,    55] loss: 0.133\n",
      "TRAIN [2,    60] loss: 0.179\n",
      "DEV [2,     5] loss: 0.506\n",
      "DEV [2,    10] loss: 0.382\n",
      "TRAIN [3,     5] loss: 0.091\n",
      "TRAIN [3,    10] loss: 0.133\n",
      "TRAIN [3,    15] loss: 0.141\n",
      "TRAIN [3,    20] loss: 0.107\n",
      "TRAIN [3,    25] loss: 0.099\n",
      "TRAIN [3,    30] loss: 0.097\n",
      "TRAIN [3,    35] loss: 0.053\n",
      "TRAIN [3,    40] loss: 0.079\n",
      "TRAIN [3,    45] loss: 0.075\n",
      "TRAIN [3,    50] loss: 0.085\n",
      "TRAIN [3,    55] loss: 0.076\n",
      "TRAIN [3,    60] loss: 0.082\n",
      "DEV [3,     5] loss: 0.484\n",
      "DEV [3,    10] loss: 0.363\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "PRINT_STEPS = 5\n",
    "\n",
    "model.to(device)\n",
    "for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dl, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = [elmt.to(device) for elmt in inputs]\n",
    "        labels = [elmt.to(device) for elmt in labels]\n",
    "        current_batch_size = inputs[0].shape[0]\n",
    "        inpt_dict = {\n",
    "            \"input_ids\":torch.cat([inputs[0], labels[0]]),\n",
    "            \"attention_mask\":torch.cat([inputs[1], labels[1]])\n",
    "        }\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inpt_dict)\n",
    "        embeddings = outputs['sentence_embedding']\n",
    "        loss = MNR_loss(embeddings[:current_batch_size], embeddings[current_batch_size:])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % PRINT_STEPS  == PRINT_STEPS-1:    # print every 2000 mini-batches\n",
    "            print(f'TRAIN [{epoch + 1}, {i + 1:5d}] loss: {running_loss / PRINT_STEPS:.3f}')\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    running_loss = 0.0       \n",
    "    for i, data in enumerate(dev_dl, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = [elmt.to(device) for elmt in inputs]\n",
    "        labels = [elmt.to(device) for elmt in labels]\n",
    "        current_batch_size = inputs[0].shape[0]\n",
    "        inpt_dict = {\n",
    "            \"input_ids\":torch.cat([inputs[0], labels[0]]),\n",
    "            \"attention_mask\":torch.cat([inputs[1], labels[1]])\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inpt_dict)\n",
    "            embeddings = outputs['sentence_embedding']\n",
    "            loss = MNR_loss(embeddings[:current_batch_size], embeddings[current_batch_size:])\n",
    "            running_loss += (loss * embeddings.shape[0]).item()\n",
    "            \n",
    "    print(f'DEV [{epoch + 1}, {i + 1:5d}] loss: {running_loss / len(dev_dl.dataset):.3f}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5e83a55-f5ec-45ee-9286-8c182c77a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = model.encode(claims.vclaim.to_list())\n",
    "# embs = model.encode(claims[[\"title\", \"subtitle\", \"vclaim\"]].apply(lambda x: f\"title: {x[0]}\\nsubtitle: {x[1]}\\nclaim: {x[2]}\", axis=1).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15a59c44-0cbd-4dad-8392-253475479a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx(connections, claims, tweets):\n",
    "    run_tweets = tweets.join(connections.set_index(\"tweet_id\"), on=\"id\", how=\"inner\")\n",
    "    run_tweets = run_tweets.join(claims.set_index(\"vclaim_id\"), on=\"claim_id\", how=\"inner\")\n",
    "    run_tweets = run_tweets[[\"tweet\", \"vclaim\"]].reset_index()\n",
    "    claim_idx = [claims.vclaim.to_list().index(t_claim) for t_claim in run_tweets.vclaim.to_list()]\n",
    "    return run_tweets, claim_idx\n",
    "\n",
    "run_tweets, claim_idx = get_idx(test_conns, claims, test_tweets)\n",
    "tweet_embs = model.encode(run_tweets.tweet.to_list())\n",
    "scores = tweet_embs @ embs.T\n",
    "ranks = [score.argsort()[::-1] for score in scores]\n",
    "\n",
    "def avg_prec(gold, rankings, n):\n",
    "    is_rel = (np.array(rankings)[:n] == gold).astype(float)\n",
    "    return (is_rel/np.arange(1,n+1)).sum()\n",
    "\n",
    "def mean_avg_prec(golds, rankings, n):\n",
    "    avg_precs = [avg_prec(gold, rlist, n) for gold, rlist in zip(golds, rankings)]\n",
    "    return np.array(avg_precs).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e54b5bfc-450b-4e75-b3d8-4c6fa29ccc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8316831683168316"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_5 = mean_avg_prec(claim_idx, ranks, 1)\n",
    "map_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d168a3e-45fc-49a3-83a5-77314280c1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inpt_dict[\"input_ids\"].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bea58ea-b6a0-416c-8ace-ac09268c9e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da5bdef7-99a6-4fda-bc87-04602f3975f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['experiments/finetune_st5_base_claims/config.ini']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"experiments/finetune_st5_base_claims/config.ini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f9262088-1c10-4953-8154-5247b8cebf29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"training\"].getfloat(\"lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bcac7839-d0b9-4b25-aa46-4cbddcae4310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sentence-transformers/sentence-t5-base'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"model\"].get(\"model_string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd9b452-1b14-417e-a427-863ae2b7d491",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
