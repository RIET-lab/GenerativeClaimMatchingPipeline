{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "653f0986-baec-456f-a937-41fb25bcc2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import utils\n",
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7093d89-7d06-4a5b-92f2-c3d50385e3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clef2021RerankedDataset(TensorDataset):\n",
    "    def __init__(self, \n",
    "                 encode_fn, \n",
    "                 claims, \n",
    "                 tweets, \n",
    "                 connections,\n",
    "                 claim_embeddings,\n",
    "                 ranks):\n",
    "        self.claim_embeddings = claim_embeddings\n",
    "        self.ranks = ranks\n",
    "        run_tweets = tweets.join(connections.set_index(\"tweet_id\"), on=\"id\", how=\"inner\")\n",
    "        run_tweets = run_tweets.join(claims.set_index(\"vclaim_id\"), on=\"claim_id\", how=\"inner\")\n",
    "        run_tweets = run_tweets[[\"tweet\", \"vclaim\"]].reset_index()\n",
    "        run_tweets[\"encoded_tweet\"] = run_tweets.tweet.apply(encode_fn)\n",
    "        self.claim_idx = [claims.vclaim.to_list().index(t_claim) for t_claim in run_tweets.vclaim.to_list()]\n",
    "        self.data = run_tweets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        Xt = self.data.encoded_tweet[index]\n",
    "        Xt = (np.array(Xt[\"input_ids\"]), np.array(Xt[\"attention_mask\"]))\n",
    "        Xe = self.claim_embeddings[self.ranks[index]]\n",
    "        Ye = self.claim_embeddings[self.claim_idx[index:index+1]]\n",
    "        return (Xt, np.concatenate([Ye, Xe], axis=0))\n",
    "    \n",
    "    \n",
    "def get_clef2021_reranked_dataloader(encode_fn, \n",
    "                            claims, \n",
    "                            tweets, \n",
    "                            connections, \n",
    "                            claim_embeddings,\n",
    "                            ranks,\n",
    "                            params={'batch_size':32, 'shuffle':True}):\n",
    "    dataset = Clef2021RerankedDataset(encode_fn, \n",
    "                              claims, \n",
    "                              tweets, \n",
    "                              connections, \n",
    "                              claim_embeddings,\n",
    "                              ranks)\n",
    "    return DataLoader(dataset, **params)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e609675-bbc1-43a4-a44a-3c2aef866d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_path = \"./experiments/finetune_st5_large_claims_negs\"\n",
    "train_neg_path = os.path.join(exp_path, \"negative_embs_train.npy\")\n",
    "dev_neg_path = os.path.join(exp_path, \"negative_embs_dev.npy\")\n",
    "emb_path = os.path.join(exp_path, \"claim_embs.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92395a0b-6e95-4737-a468-48160b480a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing ExtendedRobertaForExternalClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing ExtendedRobertaForExternalClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ExtendedRobertaForExternalClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ExtendedRobertaForExternalClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.adapter_layer.4.bias', 'roberta.encoder.adapter_layer.10.weight', 'roberta.encoder.adapter_layer.3.bias', 'roberta.encoder.adapter_layer.6.bias', 'roberta.encoder.adapter_layer.5.weight', 'roberta.encoder.adapter_layer.5.bias', 'roberta.encoder.adapter_layer.9.weight', 'roberta.encoder.adapter_layer.11.bias', 'roberta.encoder.adapter_layer.3.weight', 'roberta.encoder.adapter_layer.0.bias', 'classifier.bias', 'roberta.encoder.adapter_layer.0.weight', 'roberta.encoder.adapter_layer.9.bias', 'classifier.weight', 'roberta.encoder.adapter_layer.6.weight', 'roberta.encoder.adapter_layer.11.weight', 'roberta.encoder.adapter_layer.4.weight', 'roberta.encoder.adapter_layer.2.bias', 'roberta.encoder.adapter_layer.8.weight', 'roberta.encoder.adapter_layer.8.bias', 'roberta.encoder.adapter_layer.7.weight', 'roberta.encoder.adapter_layer.7.bias', 'roberta.encoder.adapter_layer.2.weight', 'roberta.encoder.adapter_layer.1.bias', 'roberta.encoder.adapter_layer.10.bias', 'roberta.encoder.adapter_layer.1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import extended_roberta_v2 as roberta\n",
    "from transformers import AutoTokenizer\n",
    "from functools import partial\n",
    "import importlib\n",
    "importlib.reload(roberta)\n",
    "\n",
    "MAX_LENGTH = 192\n",
    "\n",
    "model_str = \"roberta-base\"\n",
    "model = roberta.ExtendedRobertaForExternalClassification.from_pretrained(model_str)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_str)\n",
    "tokenize = partial(tokenizer, **dict(\n",
    "    truncation=True, \n",
    "    max_length=MAX_LENGTH, \n",
    "    padding=\"max_length\", \n",
    "    return_attention_mask=True\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb702fa9-ef41-4f68-b09d-ef839e2afa22",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './experiments/finetune_st5_large_claims_negs/negative_embs_train.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-84f284efc3fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mneg_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_neg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdev_neg_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_neg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mneg_embs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './experiments/finetune_st5_large_claims_negs/negative_embs_train.npy'"
     ]
    }
   ],
   "source": [
    "neg_ids = np.load(train_neg_path)\n",
    "dev_neg_ids = np.load(dev_neg_path)\n",
    "neg_embs = np.load(emb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b7c0a6-1d01-41a2-8e71-97dabce4336a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'neg_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7badaab7adb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mneg_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_embs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'neg_ids' is not defined"
     ]
    }
   ],
   "source": [
    "neg_ids.shape, neg_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffcaf466-8b8a-4fb8-83af-d11a44f6d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claim Data\n",
    "tweets, test_tweets = utils.get_tweets()\n",
    "test_tweets = test_tweets[1:]\n",
    "train_conns, dev_conns, test_conns = utils.get_qrels()\n",
    "claims = utils.get_claims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "817212db-4b4a-42b9-a577-45f85696b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dl = get_clef2021_reranked_dataloader(\n",
    "    tokenize, \n",
    "    claims, \n",
    "    tweets, \n",
    "    train_conns,\n",
    "    neg_embs,\n",
    "    neg_ids[:,:5],\n",
    "    params={'batch_size':BATCH_SIZE, 'shuffle':True})\n",
    "\n",
    "dev_dl = get_clef2021_reranked_dataloader(\n",
    "    tokenize, \n",
    "    claims, \n",
    "    tweets, \n",
    "    dev_conns,\n",
    "    neg_embs,\n",
    "    dev_neg_ids[:,:5],\n",
    "    params={'batch_size':BATCH_SIZE, 'shuffle':False}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0586d0c9-95e5-4fff-9a4e-528ce0287bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 1, 198])\n"
     ]
    }
   ],
   "source": [
    "for inputs, external_inputs in train_dl:\n",
    "    inpt_dict = {\n",
    "        \"input_ids\": inputs[0],\n",
    "        \"attention_mask\": torch.cat([torch.ones((\n",
    "            external_inputs.shape[0],\n",
    "            external_inputs.shape[1]\n",
    "        )).long(), inputs[1]], dim=1),\n",
    "        \"extended_states\": external_inputs\n",
    "    } \n",
    "    res = model(**inpt_dict)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "badd9f66-3cc1-4748-925c-4acbfa3acbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7920, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06a82cc-5fa0-4830-b500-f49d73988965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9085a49f-f159-47f1-b945-d21a17d485de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f858160-7ee5-4f38-9649-5b96f5c0febc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada634d9-d778-4c10-9b3e-3e1f1be65ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea92077f-50cb-4046-aab4-924578de875c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7ff587606c38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'train'"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import train\n",
    "importlib.reload(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72a4e5d7-b0ce-4eb8-aa5b-4ea03bc0c7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71741819-4a21-4ca8-ac95-d4bf5073579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28fdac73-ae9c-45cc-a392-1102fad07a3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN [1,     5] loss: 1.792\n",
      "TRAIN [1,    10] loss: 1.792\n",
      "TRAIN [1,    15] loss: 1.791\n",
      "TRAIN [1,    20] loss: 1.791\n",
      "TRAIN [1,    25] loss: 1.791\n",
      "TRAIN [1,    30] loss: 1.791\n",
      "DEV [1,     7] loss: 1.791\n",
      "TRAIN [2,     5] loss: 1.788\n",
      "TRAIN [2,    10] loss: 1.788\n",
      "TRAIN [2,    15] loss: 1.787\n",
      "TRAIN [2,    20] loss: 1.785\n",
      "TRAIN [2,    25] loss: 1.785\n",
      "TRAIN [2,    30] loss: 1.784\n",
      "DEV [2,     7] loss: 1.789\n",
      "TRAIN [3,     5] loss: 1.778\n",
      "TRAIN [3,    10] loss: 1.773\n",
      "TRAIN [3,    15] loss: 1.769\n",
      "TRAIN [3,    20] loss: 1.759\n",
      "TRAIN [3,    25] loss: 1.751\n",
      "TRAIN [3,    30] loss: 1.746\n",
      "DEV [3,     7] loss: 1.785\n",
      "TRAIN [4,     5] loss: 1.716\n",
      "TRAIN [4,    10] loss: 1.687\n",
      "TRAIN [4,    15] loss: 1.673\n",
      "TRAIN [4,    20] loss: 1.683\n",
      "TRAIN [4,    25] loss: 1.707\n",
      "TRAIN [4,    30] loss: 1.673\n",
      "DEV [4,     7] loss: 1.821\n",
      "TRAIN [5,     5] loss: 1.605\n",
      "TRAIN [5,    10] loss: 1.603\n",
      "TRAIN [5,    15] loss: 1.582\n",
      "TRAIN [5,    20] loss: 1.573\n",
      "TRAIN [5,    25] loss: 1.631\n",
      "TRAIN [5,    30] loss: 1.621\n",
      "DEV [5,     7] loss: 1.853\n",
      "TRAIN [6,     5] loss: 1.575\n",
      "TRAIN [6,    10] loss: 1.562\n",
      "TRAIN [6,    15] loss: 1.494\n",
      "TRAIN [6,    20] loss: 1.525\n",
      "TRAIN [6,    25] loss: 1.537\n",
      "TRAIN [6,    30] loss: 1.584\n",
      "DEV [6,     7] loss: 1.872\n",
      "TRAIN [7,     5] loss: 1.412\n",
      "TRAIN [7,    10] loss: 1.481\n",
      "TRAIN [7,    15] loss: 1.559\n",
      "TRAIN [7,    20] loss: 1.515\n",
      "TRAIN [7,    25] loss: 1.511\n",
      "TRAIN [7,    30] loss: 1.516\n",
      "DEV [7,     7] loss: 1.899\n",
      "TRAIN [8,     5] loss: 1.378\n",
      "TRAIN [8,    10] loss: 1.447\n",
      "TRAIN [8,    15] loss: 1.509\n",
      "TRAIN [8,    20] loss: 1.416\n",
      "TRAIN [8,    25] loss: 1.447\n",
      "TRAIN [8,    30] loss: 1.466\n",
      "DEV [8,     7] loss: 1.913\n",
      "TRAIN [9,     5] loss: 1.357\n",
      "TRAIN [9,    10] loss: 1.408\n",
      "TRAIN [9,    15] loss: 1.482\n",
      "TRAIN [9,    20] loss: 1.369\n",
      "TRAIN [9,    25] loss: 1.429\n",
      "TRAIN [9,    30] loss: 1.337\n",
      "DEV [9,     7] loss: 1.930\n",
      "TRAIN [10,     5] loss: 1.308\n",
      "TRAIN [10,    10] loss: 1.408\n",
      "TRAIN [10,    15] loss: 1.343\n",
      "TRAIN [10,    20] loss: 1.373\n",
      "TRAIN [10,    25] loss: 1.259\n",
      "TRAIN [10,    30] loss: 1.364\n",
      "DEV [10,     7] loss: 1.967\n",
      "TRAIN [11,     5] loss: 1.255\n",
      "TRAIN [11,    10] loss: 1.284\n",
      "TRAIN [11,    15] loss: 1.211\n",
      "TRAIN [11,    20] loss: 1.330\n",
      "TRAIN [11,    25] loss: 1.374\n",
      "TRAIN [11,    30] loss: 1.364\n",
      "DEV [11,     7] loss: 1.905\n",
      "TRAIN [12,     5] loss: 1.278\n",
      "TRAIN [12,    10] loss: 1.299\n",
      "TRAIN [12,    15] loss: 1.242\n",
      "TRAIN [12,    20] loss: 1.274\n",
      "TRAIN [12,    25] loss: 1.286\n",
      "TRAIN [12,    30] loss: 1.265\n",
      "DEV [12,     7] loss: 1.991\n",
      "TRAIN [13,     5] loss: 1.234\n",
      "TRAIN [13,    10] loss: 1.250\n",
      "TRAIN [13,    15] loss: 1.128\n",
      "TRAIN [13,    20] loss: 1.172\n",
      "TRAIN [13,    25] loss: 1.213\n",
      "TRAIN [13,    30] loss: 1.304\n",
      "DEV [13,     7] loss: 2.088\n",
      "TRAIN [14,     5] loss: 1.139\n",
      "TRAIN [14,    10] loss: 1.169\n",
      "TRAIN [14,    15] loss: 1.143\n",
      "TRAIN [14,    20] loss: 1.163\n",
      "TRAIN [14,    25] loss: 1.156\n",
      "TRAIN [14,    30] loss: 1.250\n",
      "DEV [14,     7] loss: 2.042\n",
      "TRAIN [15,     5] loss: 1.193\n",
      "TRAIN [15,    10] loss: 1.091\n",
      "TRAIN [15,    15] loss: 1.136\n",
      "TRAIN [15,    20] loss: 1.002\n",
      "TRAIN [15,    25] loss: 1.150\n",
      "TRAIN [15,    30] loss: 1.183\n",
      "DEV [15,     7] loss: 2.258\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 15\n",
    "train.train(\n",
    "    model, \n",
    "    optimizer, \n",
    "    device,\n",
    "    train_dl,\n",
    "    dev_dl,\n",
    "    epochs=EPOCHS,\n",
    "    print_steps=5,\n",
    "    adapters_only=True, \n",
    "    cls_train=True,\n",
    "    save_path=\"./cross_query/test_saved_model_adaps_5.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49dcd7cf-323c-4ba5-8989-06eb760cb1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2e9ba2a-15a6-4936-bad7-d66b5e286c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN [1,     5] loss: 1.621\n",
      "TRAIN [1,    10] loss: 1.728\n",
      "TRAIN [1,    15] loss: 1.715\n",
      "TRAIN [1,    20] loss: 1.714\n",
      "TRAIN [1,    25] loss: 1.688\n",
      "TRAIN [1,    30] loss: 1.617\n",
      "DEV [1,     7] loss: 1.831\n",
      "TRAIN [2,     5] loss: 1.549\n",
      "TRAIN [2,    10] loss: 1.360\n",
      "TRAIN [2,    15] loss: 1.369\n",
      "TRAIN [2,    20] loss: 1.268\n",
      "TRAIN [2,    25] loss: 1.354\n",
      "TRAIN [2,    30] loss: 1.333\n",
      "DEV [2,     7] loss: 2.165\n",
      "TRAIN [3,     5] loss: 1.127\n",
      "TRAIN [3,    10] loss: 1.445\n",
      "TRAIN [3,    15] loss: 1.388\n",
      "TRAIN [3,    20] loss: 1.381\n",
      "TRAIN [3,    25] loss: 1.115\n",
      "TRAIN [3,    30] loss: 1.270\n",
      "DEV [3,     7] loss: 1.899\n",
      "TRAIN [4,     5] loss: 1.196\n",
      "TRAIN [4,    10] loss: 1.154\n",
      "TRAIN [4,    15] loss: 1.164\n",
      "TRAIN [4,    20] loss: 1.131\n",
      "TRAIN [4,    25] loss: 1.118\n",
      "TRAIN [4,    30] loss: 1.100\n",
      "DEV [4,     7] loss: 1.844\n",
      "TRAIN [5,     5] loss: 1.384\n",
      "TRAIN [5,    10] loss: 1.442\n",
      "TRAIN [5,    15] loss: 1.341\n",
      "TRAIN [5,    20] loss: 1.253\n",
      "TRAIN [5,    25] loss: 1.075\n",
      "TRAIN [5,    30] loss: 1.093\n",
      "DEV [5,     7] loss: 2.085\n",
      "TRAIN [6,     5] loss: 1.088\n",
      "TRAIN [6,    10] loss: 1.040\n",
      "TRAIN [6,    15] loss: 1.039\n",
      "TRAIN [6,    20] loss: 1.082\n",
      "TRAIN [6,    25] loss: 1.048\n",
      "TRAIN [6,    30] loss: 1.005\n",
      "DEV [6,     7] loss: 2.369\n",
      "TRAIN [7,     5] loss: 0.936\n",
      "TRAIN [7,    10] loss: 0.880\n",
      "TRAIN [7,    15] loss: 0.986\n",
      "TRAIN [7,    20] loss: 1.006\n",
      "TRAIN [7,    25] loss: 0.961\n",
      "TRAIN [7,    30] loss: 0.931\n",
      "DEV [7,     7] loss: 2.294\n",
      "TRAIN [8,     5] loss: 0.922\n",
      "TRAIN [8,    10] loss: 0.838\n",
      "TRAIN [8,    15] loss: 0.845\n",
      "TRAIN [8,    20] loss: 0.794\n",
      "TRAIN [8,    25] loss: 0.983\n",
      "TRAIN [8,    30] loss: 0.846\n",
      "DEV [8,     7] loss: 2.014\n",
      "TRAIN [9,     5] loss: 0.798\n",
      "TRAIN [9,    10] loss: 0.921\n",
      "TRAIN [9,    15] loss: 0.825\n",
      "TRAIN [9,    20] loss: 0.705\n",
      "TRAIN [9,    25] loss: 0.780\n",
      "TRAIN [9,    30] loss: 0.743\n",
      "DEV [9,     7] loss: 2.356\n",
      "TRAIN [10,     5] loss: 0.735\n",
      "TRAIN [10,    10] loss: 0.767\n",
      "TRAIN [10,    15] loss: 0.752\n",
      "TRAIN [10,    20] loss: 1.014\n",
      "TRAIN [10,    25] loss: 0.921\n",
      "TRAIN [10,    30] loss: 0.894\n",
      "DEV [10,     7] loss: 2.020\n",
      "TRAIN [11,     5] loss: 0.833\n",
      "TRAIN [11,    10] loss: 0.799\n",
      "TRAIN [11,    15] loss: 0.818\n",
      "TRAIN [11,    20] loss: 0.685\n",
      "TRAIN [11,    25] loss: 0.725\n",
      "TRAIN [11,    30] loss: 0.735\n",
      "DEV [11,     7] loss: 2.200\n",
      "TRAIN [12,     5] loss: 0.685\n",
      "TRAIN [12,    10] loss: 0.654\n",
      "TRAIN [12,    15] loss: 0.547\n",
      "TRAIN [12,    20] loss: 0.677\n",
      "TRAIN [12,    25] loss: 0.878\n",
      "TRAIN [12,    30] loss: 0.645\n",
      "DEV [12,     7] loss: 2.201\n",
      "TRAIN [13,     5] loss: 0.767\n",
      "TRAIN [13,    10] loss: 0.749\n",
      "TRAIN [13,    15] loss: 0.648\n",
      "TRAIN [13,    20] loss: 0.572\n",
      "TRAIN [13,    25] loss: 0.507\n",
      "TRAIN [13,    30] loss: 0.697\n",
      "DEV [13,     7] loss: 2.646\n",
      "TRAIN [14,     5] loss: 0.537\n",
      "TRAIN [14,    10] loss: 0.463\n",
      "TRAIN [14,    15] loss: 0.535\n",
      "TRAIN [14,    20] loss: 0.489\n",
      "TRAIN [14,    25] loss: 0.540\n",
      "TRAIN [14,    30] loss: 0.581\n",
      "DEV [14,     7] loss: 2.550\n",
      "TRAIN [15,     5] loss: 0.420\n",
      "TRAIN [15,    10] loss: 0.447\n",
      "TRAIN [15,    15] loss: 0.446\n",
      "TRAIN [15,    20] loss: 0.455\n",
      "TRAIN [15,    25] loss: 0.418\n",
      "TRAIN [15,    30] loss: 0.466\n",
      "DEV [15,     7] loss: 2.854\n",
      "TRAIN [16,     5] loss: 0.377\n",
      "TRAIN [16,    10] loss: 0.334\n",
      "TRAIN [16,    15] loss: 0.425\n",
      "TRAIN [16,    20] loss: 0.457\n",
      "TRAIN [16,    25] loss: 0.396\n",
      "TRAIN [16,    30] loss: 0.343\n",
      "DEV [16,     7] loss: 3.255\n",
      "TRAIN [17,     5] loss: 0.254\n",
      "TRAIN [17,    10] loss: 0.348\n",
      "TRAIN [17,    15] loss: 0.285\n",
      "TRAIN [17,    20] loss: 0.262\n",
      "TRAIN [17,    25] loss: 0.417\n",
      "TRAIN [17,    30] loss: 0.264\n",
      "DEV [17,     7] loss: 3.318\n",
      "TRAIN [18,     5] loss: 0.223\n",
      "TRAIN [18,    10] loss: 0.288\n",
      "TRAIN [18,    15] loss: 0.277\n",
      "TRAIN [18,    20] loss: 0.210\n",
      "TRAIN [18,    25] loss: 0.229\n",
      "TRAIN [18,    30] loss: 0.318\n",
      "DEV [18,     7] loss: 3.540\n",
      "TRAIN [19,     5] loss: 0.244\n",
      "TRAIN [19,    10] loss: 0.219\n",
      "TRAIN [19,    15] loss: 0.202\n",
      "TRAIN [19,    20] loss: 0.205\n",
      "TRAIN [19,    25] loss: 0.203\n",
      "TRAIN [19,    30] loss: 0.228\n",
      "DEV [19,     7] loss: 3.253\n",
      "TRAIN [20,     5] loss: 0.306\n",
      "TRAIN [20,    10] loss: 0.381\n",
      "TRAIN [20,    15] loss: 0.275\n",
      "TRAIN [20,    20] loss: 0.292\n",
      "TRAIN [20,    25] loss: 0.292\n",
      "TRAIN [20,    30] loss: 0.256\n",
      "DEV [20,     7] loss: 3.257\n",
      "TRAIN [21,     5] loss: 0.198\n",
      "TRAIN [21,    10] loss: 0.196\n",
      "TRAIN [21,    15] loss: 0.227\n",
      "TRAIN [21,    20] loss: 0.193\n",
      "TRAIN [21,    25] loss: 0.177\n",
      "TRAIN [21,    30] loss: 0.140\n",
      "DEV [21,     7] loss: 3.836\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "train.train(\n",
    "    model, \n",
    "    optimizer, \n",
    "    device,\n",
    "    train_dl,\n",
    "    dev_dl,\n",
    "    epochs=EPOCHS+1,\n",
    "    print_steps=5,\n",
    "    adapters_only=False, \n",
    "    cls_train=True,\n",
    "    save_path=\"./cross_query/test_saved_model_5.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c75acfd7-f7a4-4f36-87f6-7810b6522006",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./cross_query/test_saved_model_5.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d69e44f5-95a8-4a4b-94df-f3601dae1e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.zeros((2,2))\n",
    "x2 = torch.ones((2,2))\n",
    "\n",
    "torch.save(x1, \"temp-delete.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28af3ce7-2db1-4994-b59a-364027721739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.load(\"temp-delete.pt\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "785c87f8-7699-43e3-ae8c-52634facbd57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(x2, \"temp-delete.pt\")\n",
    "x = torch.load(\"temp-delete.pt\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86dbeb0b-212c-43c1-bae9-37aed3c64a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_load_state_dict_into_model',\n",
       " '_load_state_dict_into_model_low_mem',\n",
       " '_prepare_model_inputs',\n",
       " '_update_model_kwargs_for_generation',\n",
       " 'base_model',\n",
       " 'base_model_prefix']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda u: \"mode\" in u.lower(), dir(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ad6283-2f42-4551-a038-a846066970b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
