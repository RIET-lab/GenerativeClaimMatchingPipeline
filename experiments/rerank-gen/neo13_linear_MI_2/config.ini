[encoder]
max_length = 256
batch_size = 24
checkpoint_steps = 1000

[training]
epochs = 4
lr = 2e-5
max_length = 256
per_chip_batch_size = 4
save_dir = training
eval_steps = 25
print_steps = 5
mask_prior = false
optimization = mutual_information
flip = true
lr_schedule = linear
; candidate_selection = experiments/candidate_selection/finetune_st5_large_claims_negs

[eval]
candidate_selection = experiments/candidate_selection/finetune_st5_large_claims_negs
n_candidates = 5

[model]
model_string = EleutherAI/gpt-neo-1.3B