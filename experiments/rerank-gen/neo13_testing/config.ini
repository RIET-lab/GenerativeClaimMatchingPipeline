[encoder]
max_length = 256
batch_size = 24
checkpoint_steps = 1000

[training]
epochs = 1
lr = 2e-5
max_length = 256
per_chip_batch_size = 2
save_dir = training
eval_steps = 25
print_steps = 5
mask_prior = true
optimization = nl3u
lr_schedule = constant
candidate_selection = experiments/candidate_selection/finetune_st5_large_claims_negs

[eval]
candidate_selection = experiments/candidate_selection/finetune_st5_large_claims_negs
n_candidates = 5

[model]
model_string = EleutherAI/gpt-neo-1.3B