[pretraining]
lr = 1e-4
batch_size = 32
adapters_only = True
max_length = 192
epochs = 10
with_negatives = False
candidate_selection_experiment = ./experiments/candidate_selection/finetune_st5_large_claims_negs

[training]
lr = 1e-5
batch_size = 16
adapters_only = False
max_length = 192
epochs = 1
candidate_selection_experiment = ./experiments/candidate_selection/finetune_st5_large_claims_negs
adapter_epochs = 50
adapter_lr = 2e-5
pretrained = True

[model]
model_string = roberta-base
version = 1
