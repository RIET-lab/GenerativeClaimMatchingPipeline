[pretraining]
lr = 1e-4
batch_size = 256
adapters_only = True
max_length = 192
epochs = 20
with_negatives = False
candidate_selection_experiment = ./experiments/candidate_selection/finetune_st5_large_claims_negs

[training]
lr = 1e-4
batch_size = 64
adapters_only = False
max_length = 192
epochs = 10
candidate_selection_experiment = ./experiments/candidate_selection/finetune_st5_large_claims_negs
adapter_epochs = 0
adapter_lr = 1e-4
pretrained = False

[model]
model_string = roberta-large
version = 3
